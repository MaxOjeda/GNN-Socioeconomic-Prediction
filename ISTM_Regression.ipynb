{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch_geometric.nn import SAGEConv, global_mean_pool\n",
    "from utils import get_feature_propagation\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from Models.GAT import GATRegression\n",
    "from Models.GraphSage import GraphSageRegression\n",
    "from Models.Transformer import TransformerRegression\n",
    "from Models.GATv2 import GATv2Regression\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[337374, 6], edge_index=[2, 641693], edge_attributes=[641693, 2], comuna=[337374], y=[337374], lat=[337374], lon=[337374])\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 1.5074, -1.1100, -1.6402,  1.2143,  1.6439,  1.6780],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]])\n",
      "tensor([-33.4416, -33.4420, -33.4429,  ..., -33.4390, -33.4389, -33.4386])\n",
      "tensor([39., 39., 39.,  ..., 42., 42., 42.])\n",
      "tensor([0.0889, 0.0889, 0.1629,  ..., 0.1620, 0.1620, 0.1620])\n",
      "tensor([[     0,      1,      2,  ..., 337372, 337373, 337373],\n",
      "        [  4237,   4241,    480,  ..., 184408, 239479,   7705]])\n",
      "tensor([[80.8500,  5.8000],\n",
      "        [18.8300,  1.3000],\n",
      "        [16.0600,  1.1000],\n",
      "        ...,\n",
      "        [33.0400,  4.1000],\n",
      "        [ 2.5300,  0.2000],\n",
      "        [ 6.4800,  0.6000]])\n"
     ]
    }
   ],
   "source": [
    "data = torch.load('Data/santiago_zero_ismt.pt')\n",
    "data.comuna = data.x[:, 8]\n",
    "data.y = data.x[:,-2]\n",
    "data.lat = data.x[:,0]\n",
    "data.lon = data.x[:,1]\n",
    "data.x = data.x[:, 2:8]\n",
    "print(data)\n",
    "print(data.x)\n",
    "print(data.lat)\n",
    "print(data.comuna)\n",
    "print(data.y)\n",
    "print(data.edge_index)\n",
    "print(data.edge_attributes)\n",
    "\n",
    "data.y = data.y.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting feature filling\n",
      "tensor([[-1.8762,  0.0880,  1.8812, -0.7552, -2.0797, -1.2788],\n",
      "        [ 0.2155, -0.1407, -0.2000,  0.7840,  0.9977,  0.7484],\n",
      "        [ 0.0507, -1.1760, -0.3934,  1.3322,  0.4679,  0.7087],\n",
      "        ...,\n",
      "        [ 1.0445, -1.0450, -1.3092,  1.1516,  1.1633,  1.0873],\n",
      "        [ 1.5074, -1.1100, -1.6402,  1.2143,  1.6439,  1.6780],\n",
      "        [ 1.2992, -0.1245, -1.1497,  0.2336,  1.0391,  0.9675]])\n",
      "Feature filling completed. It took: 2.57s\n"
     ]
    }
   ],
   "source": [
    "data.x = get_feature_propagation(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set length: 168647\n",
      "Validation set length: 84323\n",
      "Test set length: 84324\n"
     ]
    }
   ],
   "source": [
    "index_list = data.edge_index.flatten().unique().tolist()\n",
    "\n",
    "# Porcentaje de índices para cada conjunto\n",
    "train_percentage = 0.5\n",
    "val_percentage = 0.25\n",
    "\n",
    "# Dividir en conjuntos de entrenamiento y prueba\n",
    "train_index, remaining_index = train_test_split(index_list, train_size=train_percentage, random_state=27)\n",
    "\n",
    "# Dividir el resto en conjuntos de validación y prueba\n",
    "val_index, test_index = train_test_split(remaining_index, train_size=val_percentage / (1 - train_percentage), random_state=27)\n",
    "\n",
    "print(\"Training set length:\", len(train_index))\n",
    "print(\"Validation set length:\", len(val_index))\n",
    "print(\"Test set length:\", len(test_index))\n",
    "\n",
    "n_nodes, n_features = data.x.shape\n",
    "\n",
    "train_mask = torch.zeros(n_nodes, dtype=torch.bool)\n",
    "val_mask = torch.zeros(n_nodes, dtype=torch.bool)\n",
    "test_mask = torch.zeros(n_nodes, dtype=torch.bool)\n",
    "\n",
    "train_mask[train_index] = True\n",
    "val_mask[val_index] = True\n",
    "test_mask[test_index] = True\n",
    "data['train_mask'] = train_mask\n",
    "data['val_mask'] = val_mask\n",
    "data['test_mask'] = test_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GraphSage Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Train Loss: 0.1514, Val Loss: 0.0680, Val RMSE: 0.2607 Val R2: -0.1258\n",
      "Epoch: 002, Train Loss: 0.1028, Val Loss: 0.0644, Val RMSE: 0.2537 Val R2: -0.0664\n",
      "Epoch: 003, Train Loss: 0.0825, Val Loss: 0.0620, Val RMSE: 0.2490 Val R2: -0.0266\n",
      "Epoch: 004, Train Loss: 0.0715, Val Loss: 0.0600, Val RMSE: 0.2449 Val R2: 0.0066\n",
      "Epoch: 005, Train Loss: 0.0652, Val Loss: 0.0591, Val RMSE: 0.2431 Val R2: 0.0213\n",
      "Epoch: 006, Train Loss: 0.0623, Val Loss: 0.0588, Val RMSE: 0.2425 Val R2: 0.0263\n",
      "Epoch: 007, Train Loss: 0.0610, Val Loss: 0.0580, Val RMSE: 0.2409 Val R2: 0.0387\n",
      "Epoch: 008, Train Loss: 0.0599, Val Loss: 0.0569, Val RMSE: 0.2385 Val R2: 0.0582\n",
      "Epoch: 009, Train Loss: 0.0584, Val Loss: 0.0558, Val RMSE: 0.2362 Val R2: 0.0761\n",
      "Epoch: 010, Train Loss: 0.0573, Val Loss: 0.0553, Val RMSE: 0.2351 Val R2: 0.0846\n",
      "Epoch: 011, Train Loss: 0.0567, Val Loss: 0.0552, Val RMSE: 0.2349 Val R2: 0.0861\n",
      "Epoch: 012, Train Loss: 0.0566, Val Loss: 0.0551, Val RMSE: 0.2347 Val R2: 0.0874\n",
      "Epoch: 013, Train Loss: 0.0563, Val Loss: 0.0550, Val RMSE: 0.2346 Val R2: 0.0885\n",
      "Epoch: 014, Train Loss: 0.0558, Val Loss: 0.0551, Val RMSE: 0.2347 Val R2: 0.0880\n",
      "Epoch: 015, Train Loss: 0.0557, Val Loss: 0.0551, Val RMSE: 0.2347 Val R2: 0.0874\n",
      "Epoch: 016, Train Loss: 0.0556, Val Loss: 0.0551, Val RMSE: 0.2346 Val R2: 0.0882\n",
      "Epoch: 017, Train Loss: 0.0554, Val Loss: 0.0549, Val RMSE: 0.2342 Val R2: 0.0915\n",
      "Epoch: 018, Train Loss: 0.0552, Val Loss: 0.0545, Val RMSE: 0.2335 Val R2: 0.0973\n",
      "Epoch: 019, Train Loss: 0.0549, Val Loss: 0.0540, Val RMSE: 0.2324 Val R2: 0.1052\n",
      "Epoch: 020, Train Loss: 0.0544, Val Loss: 0.0535, Val RMSE: 0.2313 Val R2: 0.1137\n",
      "Epoch: 021, Train Loss: 0.0540, Val Loss: 0.0531, Val RMSE: 0.2304 Val R2: 0.1210\n",
      "Epoch: 022, Train Loss: 0.0540, Val Loss: 0.0528, Val RMSE: 0.2297 Val R2: 0.1261\n",
      "Epoch: 023, Train Loss: 0.0539, Val Loss: 0.0525, Val RMSE: 0.2292 Val R2: 0.1298\n",
      "Epoch: 024, Train Loss: 0.0538, Val Loss: 0.0524, Val RMSE: 0.2289 Val R2: 0.1325\n",
      "Epoch: 025, Train Loss: 0.0537, Val Loss: 0.0523, Val RMSE: 0.2287 Val R2: 0.1336\n",
      "Epoch: 026, Train Loss: 0.0537, Val Loss: 0.0523, Val RMSE: 0.2288 Val R2: 0.1333\n",
      "Epoch: 027, Train Loss: 0.0537, Val Loss: 0.0524, Val RMSE: 0.2288 Val R2: 0.1328\n",
      "Epoch: 028, Train Loss: 0.0536, Val Loss: 0.0524, Val RMSE: 0.2288 Val R2: 0.1330\n",
      "Epoch: 029, Train Loss: 0.0537, Val Loss: 0.0523, Val RMSE: 0.2287 Val R2: 0.1334\n",
      "Epoch: 030, Train Loss: 0.0535, Val Loss: 0.0523, Val RMSE: 0.2287 Val R2: 0.1335\n",
      "Epoch: 031, Train Loss: 0.0533, Val Loss: 0.0524, Val RMSE: 0.2288 Val R2: 0.1329\n",
      "Epoch: 032, Train Loss: 0.0535, Val Loss: 0.0524, Val RMSE: 0.2290 Val R2: 0.1315\n",
      "Epoch: 033, Train Loss: 0.0534, Val Loss: 0.0525, Val RMSE: 0.2292 Val R2: 0.1303\n",
      "Epoch: 034, Train Loss: 0.0533, Val Loss: 0.0525, Val RMSE: 0.2292 Val R2: 0.1297\n",
      "Epoch: 035, Train Loss: 0.0532, Val Loss: 0.0525, Val RMSE: 0.2292 Val R2: 0.1298\n",
      "Epoch: 036, Train Loss: 0.0532, Val Loss: 0.0525, Val RMSE: 0.2291 Val R2: 0.1304\n",
      "Epoch: 037, Train Loss: 0.0533, Val Loss: 0.0524, Val RMSE: 0.2290 Val R2: 0.1316\n",
      "Epoch: 038, Train Loss: 0.0531, Val Loss: 0.0523, Val RMSE: 0.2287 Val R2: 0.1336\n",
      "Epoch: 039, Train Loss: 0.0530, Val Loss: 0.0522, Val RMSE: 0.2284 Val R2: 0.1363\n",
      "Epoch: 040, Train Loss: 0.0529, Val Loss: 0.0520, Val RMSE: 0.2280 Val R2: 0.1390\n",
      "Epoch: 041, Train Loss: 0.0529, Val Loss: 0.0518, Val RMSE: 0.2277 Val R2: 0.1413\n",
      "Epoch: 042, Train Loss: 0.0527, Val Loss: 0.0517, Val RMSE: 0.2275 Val R2: 0.1432\n",
      "Epoch: 043, Train Loss: 0.0528, Val Loss: 0.0517, Val RMSE: 0.2273 Val R2: 0.1445\n",
      "Epoch: 044, Train Loss: 0.0526, Val Loss: 0.0516, Val RMSE: 0.2271 Val R2: 0.1457\n",
      "Epoch: 045, Train Loss: 0.0527, Val Loss: 0.0515, Val RMSE: 0.2270 Val R2: 0.1468\n",
      "Epoch: 046, Train Loss: 0.0526, Val Loss: 0.0514, Val RMSE: 0.2268 Val R2: 0.1481\n",
      "Epoch: 047, Train Loss: 0.0526, Val Loss: 0.0514, Val RMSE: 0.2266 Val R2: 0.1493\n",
      "Epoch: 048, Train Loss: 0.0525, Val Loss: 0.0513, Val RMSE: 0.2266 Val R2: 0.1498\n",
      "Epoch: 049, Train Loss: 0.0524, Val Loss: 0.0513, Val RMSE: 0.2266 Val R2: 0.1498\n",
      "Epoch: 050, Train Loss: 0.0523, Val Loss: 0.0513, Val RMSE: 0.2266 Val R2: 0.1496\n",
      "Epoch: 051, Train Loss: 0.0522, Val Loss: 0.0514, Val RMSE: 0.2266 Val R2: 0.1493\n",
      "Epoch: 052, Train Loss: 0.0524, Val Loss: 0.0514, Val RMSE: 0.2267 Val R2: 0.1489\n",
      "Epoch: 053, Train Loss: 0.0522, Val Loss: 0.0514, Val RMSE: 0.2267 Val R2: 0.1491\n",
      "Epoch: 054, Train Loss: 0.0522, Val Loss: 0.0513, Val RMSE: 0.2265 Val R2: 0.1501\n",
      "Epoch: 055, Train Loss: 0.0522, Val Loss: 0.0512, Val RMSE: 0.2263 Val R2: 0.1516\n",
      "Epoch: 056, Train Loss: 0.0521, Val Loss: 0.0511, Val RMSE: 0.2261 Val R2: 0.1534\n",
      "Epoch: 057, Train Loss: 0.0520, Val Loss: 0.0510, Val RMSE: 0.2259 Val R2: 0.1552\n",
      "Epoch: 058, Train Loss: 0.0520, Val Loss: 0.0509, Val RMSE: 0.2256 Val R2: 0.1568\n",
      "Epoch: 059, Train Loss: 0.0520, Val Loss: 0.0508, Val RMSE: 0.2255 Val R2: 0.1582\n",
      "Epoch: 060, Train Loss: 0.0521, Val Loss: 0.0508, Val RMSE: 0.2253 Val R2: 0.1590\n",
      "Epoch: 061, Train Loss: 0.0519, Val Loss: 0.0508, Val RMSE: 0.2253 Val R2: 0.1594\n",
      "Epoch: 062, Train Loss: 0.0519, Val Loss: 0.0507, Val RMSE: 0.2252 Val R2: 0.1597\n",
      "Epoch: 063, Train Loss: 0.0519, Val Loss: 0.0507, Val RMSE: 0.2252 Val R2: 0.1602\n",
      "Epoch: 064, Train Loss: 0.0519, Val Loss: 0.0507, Val RMSE: 0.2251 Val R2: 0.1609\n",
      "Epoch: 065, Train Loss: 0.0517, Val Loss: 0.0506, Val RMSE: 0.2250 Val R2: 0.1615\n",
      "Epoch: 066, Train Loss: 0.0517, Val Loss: 0.0506, Val RMSE: 0.2249 Val R2: 0.1621\n",
      "Epoch: 067, Train Loss: 0.0517, Val Loss: 0.0506, Val RMSE: 0.2249 Val R2: 0.1626\n",
      "Epoch: 068, Train Loss: 0.0518, Val Loss: 0.0506, Val RMSE: 0.2248 Val R2: 0.1627\n",
      "Epoch: 069, Train Loss: 0.0515, Val Loss: 0.0505, Val RMSE: 0.2248 Val R2: 0.1630\n",
      "Epoch: 070, Train Loss: 0.0515, Val Loss: 0.0505, Val RMSE: 0.2247 Val R2: 0.1637\n",
      "Epoch: 071, Train Loss: 0.0515, Val Loss: 0.0504, Val RMSE: 0.2246 Val R2: 0.1646\n",
      "Epoch: 072, Train Loss: 0.0516, Val Loss: 0.0504, Val RMSE: 0.2244 Val R2: 0.1658\n",
      "Epoch: 073, Train Loss: 0.0515, Val Loss: 0.0503, Val RMSE: 0.2243 Val R2: 0.1669\n",
      "Epoch: 074, Train Loss: 0.0514, Val Loss: 0.0502, Val RMSE: 0.2241 Val R2: 0.1680\n",
      "Epoch: 075, Train Loss: 0.0514, Val Loss: 0.0502, Val RMSE: 0.2240 Val R2: 0.1689\n",
      "Epoch: 076, Train Loss: 0.0514, Val Loss: 0.0501, Val RMSE: 0.2239 Val R2: 0.1697\n",
      "Epoch: 077, Train Loss: 0.0513, Val Loss: 0.0501, Val RMSE: 0.2238 Val R2: 0.1706\n",
      "Epoch: 078, Train Loss: 0.0513, Val Loss: 0.0500, Val RMSE: 0.2237 Val R2: 0.1716\n",
      "Epoch: 079, Train Loss: 0.0513, Val Loss: 0.0500, Val RMSE: 0.2235 Val R2: 0.1725\n",
      "Epoch: 080, Train Loss: 0.0512, Val Loss: 0.0499, Val RMSE: 0.2234 Val R2: 0.1732\n",
      "Epoch: 081, Train Loss: 0.0511, Val Loss: 0.0499, Val RMSE: 0.2233 Val R2: 0.1742\n",
      "Epoch: 082, Train Loss: 0.0513, Val Loss: 0.0498, Val RMSE: 0.2232 Val R2: 0.1753\n",
      "Epoch: 083, Train Loss: 0.0511, Val Loss: 0.0498, Val RMSE: 0.2231 Val R2: 0.1759\n",
      "Epoch: 084, Train Loss: 0.0510, Val Loss: 0.0497, Val RMSE: 0.2230 Val R2: 0.1763\n",
      "Epoch: 085, Train Loss: 0.0510, Val Loss: 0.0497, Val RMSE: 0.2230 Val R2: 0.1766\n",
      "Epoch: 086, Train Loss: 0.0510, Val Loss: 0.0497, Val RMSE: 0.2229 Val R2: 0.1774\n",
      "Epoch: 087, Train Loss: 0.0509, Val Loss: 0.0496, Val RMSE: 0.2227 Val R2: 0.1789\n",
      "Epoch: 088, Train Loss: 0.0508, Val Loss: 0.0495, Val RMSE: 0.2225 Val R2: 0.1804\n",
      "Epoch: 089, Train Loss: 0.0509, Val Loss: 0.0494, Val RMSE: 0.2223 Val R2: 0.1813\n",
      "Epoch: 090, Train Loss: 0.0509, Val Loss: 0.0494, Val RMSE: 0.2222 Val R2: 0.1821\n",
      "Epoch: 091, Train Loss: 0.0507, Val Loss: 0.0493, Val RMSE: 0.2221 Val R2: 0.1829\n",
      "Epoch: 092, Train Loss: 0.0508, Val Loss: 0.0493, Val RMSE: 0.2221 Val R2: 0.1833\n",
      "Epoch: 093, Train Loss: 0.0507, Val Loss: 0.0493, Val RMSE: 0.2220 Val R2: 0.1838\n",
      "Epoch: 094, Train Loss: 0.0507, Val Loss: 0.0492, Val RMSE: 0.2218 Val R2: 0.1855\n",
      "Epoch: 095, Train Loss: 0.0506, Val Loss: 0.0491, Val RMSE: 0.2215 Val R2: 0.1876\n",
      "Epoch: 096, Train Loss: 0.0504, Val Loss: 0.0490, Val RMSE: 0.2213 Val R2: 0.1890\n",
      "Epoch: 097, Train Loss: 0.0505, Val Loss: 0.0489, Val RMSE: 0.2212 Val R2: 0.1895\n",
      "Epoch: 098, Train Loss: 0.0504, Val Loss: 0.0490, Val RMSE: 0.2213 Val R2: 0.1892\n",
      "Epoch: 099, Train Loss: 0.0505, Val Loss: 0.0489, Val RMSE: 0.2211 Val R2: 0.1903\n",
      "Epoch: 100, Train Loss: 0.0503, Val Loss: 0.0487, Val RMSE: 0.2208 Val R2: 0.1929\n",
      "Epoch: 101, Train Loss: 0.0502, Val Loss: 0.0486, Val RMSE: 0.2206 Val R2: 0.1944\n",
      "Epoch: 102, Train Loss: 0.0502, Val Loss: 0.0486, Val RMSE: 0.2206 Val R2: 0.1943\n",
      "Epoch: 103, Train Loss: 0.0502, Val Loss: 0.0485, Val RMSE: 0.2203 Val R2: 0.1961\n",
      "Epoch: 104, Train Loss: 0.0502, Val Loss: 0.0485, Val RMSE: 0.2202 Val R2: 0.1969\n",
      "Epoch: 105, Train Loss: 0.0502, Val Loss: 0.0485, Val RMSE: 0.2203 Val R2: 0.1961\n",
      "Epoch: 106, Train Loss: 0.0501, Val Loss: 0.0485, Val RMSE: 0.2203 Val R2: 0.1964\n",
      "Epoch: 107, Train Loss: 0.0500, Val Loss: 0.0484, Val RMSE: 0.2200 Val R2: 0.1984\n",
      "Epoch: 108, Train Loss: 0.0500, Val Loss: 0.0484, Val RMSE: 0.2199 Val R2: 0.1991\n",
      "Epoch: 109, Train Loss: 0.0501, Val Loss: 0.0484, Val RMSE: 0.2199 Val R2: 0.1989\n",
      "Epoch: 110, Train Loss: 0.0499, Val Loss: 0.0482, Val RMSE: 0.2195 Val R2: 0.2017\n",
      "Epoch: 111, Train Loss: 0.0499, Val Loss: 0.0481, Val RMSE: 0.2194 Val R2: 0.2029\n",
      "Epoch: 112, Train Loss: 0.0498, Val Loss: 0.0482, Val RMSE: 0.2195 Val R2: 0.2020\n",
      "Epoch: 113, Train Loss: 0.0498, Val Loss: 0.0481, Val RMSE: 0.2194 Val R2: 0.2030\n",
      "Epoch: 114, Train Loss: 0.0497, Val Loss: 0.0480, Val RMSE: 0.2190 Val R2: 0.2053\n",
      "Epoch: 115, Train Loss: 0.0496, Val Loss: 0.0480, Val RMSE: 0.2190 Val R2: 0.2056\n",
      "Epoch: 116, Train Loss: 0.0496, Val Loss: 0.0480, Val RMSE: 0.2190 Val R2: 0.2055\n",
      "Epoch: 117, Train Loss: 0.0494, Val Loss: 0.0478, Val RMSE: 0.2186 Val R2: 0.2082\n",
      "Epoch: 118, Train Loss: 0.0495, Val Loss: 0.0477, Val RMSE: 0.2185 Val R2: 0.2095\n",
      "Epoch: 119, Train Loss: 0.0495, Val Loss: 0.0477, Val RMSE: 0.2184 Val R2: 0.2097\n",
      "Epoch: 120, Train Loss: 0.0493, Val Loss: 0.0476, Val RMSE: 0.2183 Val R2: 0.2111\n",
      "Epoch: 121, Train Loss: 0.0494, Val Loss: 0.0476, Val RMSE: 0.2181 Val R2: 0.2119\n",
      "Epoch: 122, Train Loss: 0.0493, Val Loss: 0.0475, Val RMSE: 0.2180 Val R2: 0.2133\n",
      "Epoch: 123, Train Loss: 0.0493, Val Loss: 0.0475, Val RMSE: 0.2180 Val R2: 0.2131\n",
      "Epoch: 124, Train Loss: 0.0493, Val Loss: 0.0475, Val RMSE: 0.2179 Val R2: 0.2140\n",
      "Epoch: 125, Train Loss: 0.0494, Val Loss: 0.0475, Val RMSE: 0.2179 Val R2: 0.2133\n",
      "Epoch: 126, Train Loss: 0.0492, Val Loss: 0.0475, Val RMSE: 0.2180 Val R2: 0.2131\n",
      "Epoch: 127, Train Loss: 0.0491, Val Loss: 0.0474, Val RMSE: 0.2177 Val R2: 0.2152\n",
      "Epoch: 128, Train Loss: 0.0491, Val Loss: 0.0473, Val RMSE: 0.2175 Val R2: 0.2167\n",
      "Epoch: 129, Train Loss: 0.0491, Val Loss: 0.0473, Val RMSE: 0.2176 Val R2: 0.2159\n",
      "Epoch: 130, Train Loss: 0.0491, Val Loss: 0.0473, Val RMSE: 0.2174 Val R2: 0.2171\n",
      "Epoch: 131, Train Loss: 0.0490, Val Loss: 0.0472, Val RMSE: 0.2173 Val R2: 0.2183\n",
      "Epoch: 132, Train Loss: 0.0490, Val Loss: 0.0473, Val RMSE: 0.2175 Val R2: 0.2165\n",
      "Epoch: 133, Train Loss: 0.0489, Val Loss: 0.0473, Val RMSE: 0.2174 Val R2: 0.2174\n",
      "Epoch: 134, Train Loss: 0.0490, Val Loss: 0.0471, Val RMSE: 0.2171 Val R2: 0.2192\n",
      "Epoch: 135, Train Loss: 0.0489, Val Loss: 0.0472, Val RMSE: 0.2172 Val R2: 0.2190\n",
      "Epoch: 136, Train Loss: 0.0489, Val Loss: 0.0471, Val RMSE: 0.2171 Val R2: 0.2197\n",
      "Epoch: 137, Train Loss: 0.0488, Val Loss: 0.0470, Val RMSE: 0.2169 Val R2: 0.2209\n",
      "Epoch: 138, Train Loss: 0.0489, Val Loss: 0.0471, Val RMSE: 0.2170 Val R2: 0.2205\n",
      "Epoch: 139, Train Loss: 0.0488, Val Loss: 0.0470, Val RMSE: 0.2169 Val R2: 0.2208\n",
      "Epoch: 140, Train Loss: 0.0488, Val Loss: 0.0471, Val RMSE: 0.2170 Val R2: 0.2201\n",
      "Epoch: 141, Train Loss: 0.0486, Val Loss: 0.0470, Val RMSE: 0.2168 Val R2: 0.2212\n",
      "Epoch: 142, Train Loss: 0.0486, Val Loss: 0.0469, Val RMSE: 0.2167 Val R2: 0.2225\n",
      "Epoch: 143, Train Loss: 0.0487, Val Loss: 0.0470, Val RMSE: 0.2167 Val R2: 0.2221\n",
      "Epoch: 144, Train Loss: 0.0486, Val Loss: 0.0470, Val RMSE: 0.2167 Val R2: 0.2223\n",
      "Epoch: 145, Train Loss: 0.0486, Val Loss: 0.0469, Val RMSE: 0.2166 Val R2: 0.2234\n",
      "Epoch: 146, Train Loss: 0.0485, Val Loss: 0.0469, Val RMSE: 0.2165 Val R2: 0.2235\n",
      "Epoch: 147, Train Loss: 0.0486, Val Loss: 0.0469, Val RMSE: 0.2165 Val R2: 0.2235\n",
      "Epoch: 148, Train Loss: 0.0485, Val Loss: 0.0468, Val RMSE: 0.2163 Val R2: 0.2250\n",
      "Epoch: 149, Train Loss: 0.0485, Val Loss: 0.0468, Val RMSE: 0.2162 Val R2: 0.2256\n",
      "Epoch: 150, Train Loss: 0.0485, Val Loss: 0.0468, Val RMSE: 0.2163 Val R2: 0.2250\n",
      "Epoch: 151, Train Loss: 0.0484, Val Loss: 0.0468, Val RMSE: 0.2163 Val R2: 0.2249\n",
      "Epoch: 152, Train Loss: 0.0485, Val Loss: 0.0468, Val RMSE: 0.2162 Val R2: 0.2256\n",
      "Epoch: 153, Train Loss: 0.0485, Val Loss: 0.0469, Val RMSE: 0.2165 Val R2: 0.2239\n",
      "Epoch: 154, Train Loss: 0.0484, Val Loss: 0.0467, Val RMSE: 0.2161 Val R2: 0.2264\n",
      "Epoch: 155, Train Loss: 0.0483, Val Loss: 0.0466, Val RMSE: 0.2160 Val R2: 0.2274\n",
      "Epoch: 156, Train Loss: 0.0484, Val Loss: 0.0467, Val RMSE: 0.2161 Val R2: 0.2269\n",
      "Epoch: 157, Train Loss: 0.0484, Val Loss: 0.0466, Val RMSE: 0.2159 Val R2: 0.2279\n",
      "Epoch: 158, Train Loss: 0.0483, Val Loss: 0.0467, Val RMSE: 0.2160 Val R2: 0.2273\n",
      "Epoch: 159, Train Loss: 0.0483, Val Loss: 0.0467, Val RMSE: 0.2161 Val R2: 0.2265\n",
      "Epoch: 160, Train Loss: 0.0483, Val Loss: 0.0466, Val RMSE: 0.2159 Val R2: 0.2279\n",
      "Epoch: 161, Train Loss: 0.0482, Val Loss: 0.0465, Val RMSE: 0.2157 Val R2: 0.2295\n",
      "Epoch: 162, Train Loss: 0.0483, Val Loss: 0.0466, Val RMSE: 0.2158 Val R2: 0.2288\n",
      "Epoch: 163, Train Loss: 0.0483, Val Loss: 0.0465, Val RMSE: 0.2156 Val R2: 0.2300\n",
      "Epoch: 164, Train Loss: 0.0482, Val Loss: 0.0466, Val RMSE: 0.2158 Val R2: 0.2288\n",
      "Epoch: 165, Train Loss: 0.0482, Val Loss: 0.0466, Val RMSE: 0.2159 Val R2: 0.2283\n",
      "Epoch: 166, Train Loss: 0.0481, Val Loss: 0.0465, Val RMSE: 0.2156 Val R2: 0.2300\n",
      "Epoch: 167, Train Loss: 0.0482, Val Loss: 0.0465, Val RMSE: 0.2156 Val R2: 0.2303\n",
      "Epoch: 168, Train Loss: 0.0482, Val Loss: 0.0465, Val RMSE: 0.2157 Val R2: 0.2293\n",
      "Epoch: 169, Train Loss: 0.0483, Val Loss: 0.0465, Val RMSE: 0.2157 Val R2: 0.2294\n",
      "Epoch: 170, Train Loss: 0.0482, Val Loss: 0.0465, Val RMSE: 0.2157 Val R2: 0.2296\n",
      "Epoch: 171, Train Loss: 0.0481, Val Loss: 0.0467, Val RMSE: 0.2160 Val R2: 0.2274\n",
      "Epoch: 172, Train Loss: 0.0482, Val Loss: 0.0464, Val RMSE: 0.2155 Val R2: 0.2311\n",
      "Epoch: 173, Train Loss: 0.0482, Val Loss: 0.0464, Val RMSE: 0.2154 Val R2: 0.2319\n",
      "Epoch: 174, Train Loss: 0.0481, Val Loss: 0.0465, Val RMSE: 0.2157 Val R2: 0.2293\n",
      "Epoch: 175, Train Loss: 0.0482, Val Loss: 0.0465, Val RMSE: 0.2156 Val R2: 0.2301\n",
      "Epoch: 176, Train Loss: 0.0481, Val Loss: 0.0465, Val RMSE: 0.2156 Val R2: 0.2300\n",
      "Epoch: 177, Train Loss: 0.0481, Val Loss: 0.0466, Val RMSE: 0.2159 Val R2: 0.2279\n",
      "Epoch: 178, Train Loss: 0.0482, Val Loss: 0.0463, Val RMSE: 0.2152 Val R2: 0.2327\n",
      "Epoch: 179, Train Loss: 0.0480, Val Loss: 0.0464, Val RMSE: 0.2154 Val R2: 0.2319\n",
      "Epoch: 180, Train Loss: 0.0481, Val Loss: 0.0466, Val RMSE: 0.2158 Val R2: 0.2290\n",
      "Epoch: 181, Train Loss: 0.0481, Val Loss: 0.0464, Val RMSE: 0.2153 Val R2: 0.2320\n",
      "Epoch: 182, Train Loss: 0.0481, Val Loss: 0.0465, Val RMSE: 0.2157 Val R2: 0.2297\n",
      "Epoch: 183, Train Loss: 0.0480, Val Loss: 0.0463, Val RMSE: 0.2152 Val R2: 0.2331\n",
      "Epoch: 184, Train Loss: 0.0480, Val Loss: 0.0463, Val RMSE: 0.2152 Val R2: 0.2333\n",
      "Epoch: 185, Train Loss: 0.0481, Val Loss: 0.0465, Val RMSE: 0.2155 Val R2: 0.2307\n",
      "Epoch: 186, Train Loss: 0.0480, Val Loss: 0.0464, Val RMSE: 0.2154 Val R2: 0.2317\n",
      "Epoch: 187, Train Loss: 0.0479, Val Loss: 0.0464, Val RMSE: 0.2154 Val R2: 0.2316\n",
      "Epoch: 188, Train Loss: 0.0480, Val Loss: 0.0463, Val RMSE: 0.2152 Val R2: 0.2332\n",
      "Epoch: 189, Train Loss: 0.0480, Val Loss: 0.0463, Val RMSE: 0.2151 Val R2: 0.2338\n",
      "Epoch: 190, Train Loss: 0.0479, Val Loss: 0.0463, Val RMSE: 0.2152 Val R2: 0.2328\n",
      "Epoch: 191, Train Loss: 0.0479, Val Loss: 0.0464, Val RMSE: 0.2153 Val R2: 0.2320\n",
      "Epoch: 192, Train Loss: 0.0478, Val Loss: 0.0464, Val RMSE: 0.2153 Val R2: 0.2322\n",
      "Epoch: 193, Train Loss: 0.0479, Val Loss: 0.0463, Val RMSE: 0.2151 Val R2: 0.2334\n",
      "Epoch: 194, Train Loss: 0.0478, Val Loss: 0.0462, Val RMSE: 0.2150 Val R2: 0.2341\n",
      "Epoch: 195, Train Loss: 0.0479, Val Loss: 0.0463, Val RMSE: 0.2153 Val R2: 0.2325\n",
      "Epoch: 196, Train Loss: 0.0479, Val Loss: 0.0464, Val RMSE: 0.2153 Val R2: 0.2321\n",
      "Epoch: 197, Train Loss: 0.0480, Val Loss: 0.0463, Val RMSE: 0.2153 Val R2: 0.2324\n",
      "Epoch: 198, Train Loss: 0.0479, Val Loss: 0.0462, Val RMSE: 0.2150 Val R2: 0.2346\n",
      "Epoch: 199, Train Loss: 0.0479, Val Loss: 0.0462, Val RMSE: 0.2150 Val R2: 0.2343\n",
      "Epoch: 200, Train Loss: 0.0478, Val Loss: 0.0463, Val RMSE: 0.2152 Val R2: 0.2333\n",
      "Epoch: 201, Train Loss: 0.0478, Val Loss: 0.0462, Val RMSE: 0.2150 Val R2: 0.2342\n",
      "Epoch: 202, Train Loss: 0.0479, Val Loss: 0.0463, Val RMSE: 0.2151 Val R2: 0.2334\n",
      "Epoch: 203, Train Loss: 0.0478, Val Loss: 0.0462, Val RMSE: 0.2149 Val R2: 0.2348\n",
      "Epoch: 204, Train Loss: 0.0478, Val Loss: 0.0462, Val RMSE: 0.2149 Val R2: 0.2350\n",
      "Epoch: 205, Train Loss: 0.0478, Val Loss: 0.0464, Val RMSE: 0.2154 Val R2: 0.2316\n",
      "Epoch: 206, Train Loss: 0.0478, Val Loss: 0.0462, Val RMSE: 0.2149 Val R2: 0.2354\n",
      "Epoch: 207, Train Loss: 0.0478, Val Loss: 0.0463, Val RMSE: 0.2151 Val R2: 0.2334\n",
      "Epoch: 208, Train Loss: 0.0477, Val Loss: 0.0463, Val RMSE: 0.2151 Val R2: 0.2338\n",
      "Epoch: 209, Train Loss: 0.0478, Val Loss: 0.0462, Val RMSE: 0.2149 Val R2: 0.2354\n",
      "Epoch: 210, Train Loss: 0.0479, Val Loss: 0.0463, Val RMSE: 0.2152 Val R2: 0.2329\n",
      "Epoch: 211, Train Loss: 0.0477, Val Loss: 0.0461, Val RMSE: 0.2148 Val R2: 0.2359\n",
      "Epoch: 212, Train Loss: 0.0478, Val Loss: 0.0463, Val RMSE: 0.2153 Val R2: 0.2325\n",
      "Epoch: 213, Train Loss: 0.0478, Val Loss: 0.0462, Val RMSE: 0.2150 Val R2: 0.2342\n",
      "Epoch: 214, Train Loss: 0.0478, Val Loss: 0.0462, Val RMSE: 0.2150 Val R2: 0.2343\n",
      "Epoch: 215, Train Loss: 0.0477, Val Loss: 0.0461, Val RMSE: 0.2148 Val R2: 0.2360\n",
      "Epoch: 216, Train Loss: 0.0477, Val Loss: 0.0462, Val RMSE: 0.2148 Val R2: 0.2356\n",
      "Epoch: 217, Train Loss: 0.0476, Val Loss: 0.0462, Val RMSE: 0.2150 Val R2: 0.2345\n",
      "Epoch: 218, Train Loss: 0.0477, Val Loss: 0.0461, Val RMSE: 0.2148 Val R2: 0.2358\n",
      "Epoch: 219, Train Loss: 0.0477, Val Loss: 0.0462, Val RMSE: 0.2149 Val R2: 0.2349\n",
      "Epoch: 220, Train Loss: 0.0477, Val Loss: 0.0461, Val RMSE: 0.2147 Val R2: 0.2368\n",
      "Epoch: 221, Train Loss: 0.0477, Val Loss: 0.0462, Val RMSE: 0.2150 Val R2: 0.2343\n",
      "Epoch: 222, Train Loss: 0.0476, Val Loss: 0.0461, Val RMSE: 0.2148 Val R2: 0.2358\n",
      "Epoch: 223, Train Loss: 0.0477, Val Loss: 0.0464, Val RMSE: 0.2154 Val R2: 0.2319\n",
      "Epoch: 224, Train Loss: 0.0478, Val Loss: 0.0461, Val RMSE: 0.2147 Val R2: 0.2364\n",
      "Epoch: 225, Train Loss: 0.0476, Val Loss: 0.0460, Val RMSE: 0.2146 Val R2: 0.2375\n",
      "Epoch: 226, Train Loss: 0.0477, Val Loss: 0.0463, Val RMSE: 0.2152 Val R2: 0.2327\n",
      "Epoch: 227, Train Loss: 0.0477, Val Loss: 0.0461, Val RMSE: 0.2147 Val R2: 0.2367\n",
      "Epoch: 228, Train Loss: 0.0476, Val Loss: 0.0461, Val RMSE: 0.2148 Val R2: 0.2357\n",
      "Epoch: 229, Train Loss: 0.0476, Val Loss: 0.0462, Val RMSE: 0.2149 Val R2: 0.2350\n",
      "Epoch: 230, Train Loss: 0.0478, Val Loss: 0.0460, Val RMSE: 0.2146 Val R2: 0.2374\n",
      "Epoch: 231, Train Loss: 0.0476, Val Loss: 0.0464, Val RMSE: 0.2153 Val R2: 0.2321\n",
      "Epoch: 232, Train Loss: 0.0478, Val Loss: 0.0461, Val RMSE: 0.2147 Val R2: 0.2369\n",
      "Epoch: 233, Train Loss: 0.0476, Val Loss: 0.0461, Val RMSE: 0.2147 Val R2: 0.2364\n",
      "Epoch: 234, Train Loss: 0.0476, Val Loss: 0.0461, Val RMSE: 0.2148 Val R2: 0.2359\n",
      "Epoch: 235, Train Loss: 0.0477, Val Loss: 0.0460, Val RMSE: 0.2145 Val R2: 0.2381\n",
      "Epoch: 236, Train Loss: 0.0477, Val Loss: 0.0462, Val RMSE: 0.2150 Val R2: 0.2345\n",
      "Epoch: 237, Train Loss: 0.0476, Val Loss: 0.0461, Val RMSE: 0.2147 Val R2: 0.2366\n",
      "Epoch: 238, Train Loss: 0.0476, Val Loss: 0.0460, Val RMSE: 0.2145 Val R2: 0.2382\n",
      "Epoch: 239, Train Loss: 0.0476, Val Loss: 0.0462, Val RMSE: 0.2149 Val R2: 0.2355\n",
      "Epoch: 240, Train Loss: 0.0477, Val Loss: 0.0460, Val RMSE: 0.2145 Val R2: 0.2378\n",
      "Epoch: 241, Train Loss: 0.0476, Val Loss: 0.0461, Val RMSE: 0.2147 Val R2: 0.2365\n",
      "Epoch: 242, Train Loss: 0.0475, Val Loss: 0.0461, Val RMSE: 0.2146 Val R2: 0.2372\n",
      "Epoch: 243, Train Loss: 0.0475, Val Loss: 0.0460, Val RMSE: 0.2145 Val R2: 0.2377\n",
      "Epoch: 244, Train Loss: 0.0475, Val Loss: 0.0460, Val RMSE: 0.2146 Val R2: 0.2374\n",
      "Epoch: 245, Train Loss: 0.0477, Val Loss: 0.0460, Val RMSE: 0.2145 Val R2: 0.2381\n",
      "Epoch: 246, Train Loss: 0.0475, Val Loss: 0.0461, Val RMSE: 0.2146 Val R2: 0.2372\n",
      "Epoch: 247, Train Loss: 0.0474, Val Loss: 0.0461, Val RMSE: 0.2146 Val R2: 0.2371\n",
      "Epoch: 248, Train Loss: 0.0475, Val Loss: 0.0460, Val RMSE: 0.2144 Val R2: 0.2384\n",
      "Epoch: 249, Train Loss: 0.0476, Val Loss: 0.0459, Val RMSE: 0.2143 Val R2: 0.2391\n",
      "Epoch: 250, Train Loss: 0.0474, Val Loss: 0.0461, Val RMSE: 0.2147 Val R2: 0.2366\n",
      "Epoch: 251, Train Loss: 0.0475, Val Loss: 0.0460, Val RMSE: 0.2144 Val R2: 0.2387\n",
      "Epoch: 252, Train Loss: 0.0474, Val Loss: 0.0460, Val RMSE: 0.2146 Val R2: 0.2374\n",
      "Epoch: 253, Train Loss: 0.0475, Val Loss: 0.0459, Val RMSE: 0.2143 Val R2: 0.2394\n",
      "Epoch: 254, Train Loss: 0.0475, Val Loss: 0.0461, Val RMSE: 0.2148 Val R2: 0.2359\n",
      "Epoch: 255, Train Loss: 0.0475, Val Loss: 0.0459, Val RMSE: 0.2143 Val R2: 0.2397\n",
      "Epoch: 256, Train Loss: 0.0475, Val Loss: 0.0462, Val RMSE: 0.2150 Val R2: 0.2345\n",
      "Epoch: 257, Train Loss: 0.0475, Val Loss: 0.0459, Val RMSE: 0.2142 Val R2: 0.2401\n",
      "Epoch: 258, Train Loss: 0.0473, Val Loss: 0.0461, Val RMSE: 0.2146 Val R2: 0.2372\n",
      "Epoch: 259, Train Loss: 0.0474, Val Loss: 0.0460, Val RMSE: 0.2145 Val R2: 0.2382\n",
      "Epoch: 260, Train Loss: 0.0474, Val Loss: 0.0459, Val RMSE: 0.2143 Val R2: 0.2391\n",
      "Epoch: 261, Train Loss: 0.0474, Val Loss: 0.0461, Val RMSE: 0.2146 Val R2: 0.2370\n",
      "Epoch: 262, Train Loss: 0.0475, Val Loss: 0.0459, Val RMSE: 0.2143 Val R2: 0.2396\n",
      "Epoch: 263, Train Loss: 0.0475, Val Loss: 0.0464, Val RMSE: 0.2155 Val R2: 0.2309\n",
      "Epoch: 264, Train Loss: 0.0476, Val Loss: 0.0459, Val RMSE: 0.2142 Val R2: 0.2400\n",
      "Epoch: 265, Train Loss: 0.0475, Val Loss: 0.0459, Val RMSE: 0.2142 Val R2: 0.2399\n",
      "Epoch: 266, Train Loss: 0.0474, Val Loss: 0.0461, Val RMSE: 0.2148 Val R2: 0.2361\n",
      "Epoch: 267, Train Loss: 0.0476, Val Loss: 0.0459, Val RMSE: 0.2142 Val R2: 0.2402\n",
      "Epoch: 268, Train Loss: 0.0473, Val Loss: 0.0459, Val RMSE: 0.2143 Val R2: 0.2393\n",
      "Epoch: 269, Train Loss: 0.0474, Val Loss: 0.0461, Val RMSE: 0.2147 Val R2: 0.2366\n",
      "Epoch: 270, Train Loss: 0.0473, Val Loss: 0.0459, Val RMSE: 0.2141 Val R2: 0.2406\n",
      "Epoch: 271, Train Loss: 0.0474, Val Loss: 0.0462, Val RMSE: 0.2150 Val R2: 0.2345\n",
      "Epoch: 272, Train Loss: 0.0476, Val Loss: 0.0458, Val RMSE: 0.2140 Val R2: 0.2414\n",
      "Epoch: 273, Train Loss: 0.0474, Val Loss: 0.0460, Val RMSE: 0.2145 Val R2: 0.2381\n",
      "Epoch: 274, Train Loss: 0.0473, Val Loss: 0.0460, Val RMSE: 0.2144 Val R2: 0.2388\n",
      "Epoch: 275, Train Loss: 0.0473, Val Loss: 0.0458, Val RMSE: 0.2140 Val R2: 0.2413\n",
      "Epoch: 276, Train Loss: 0.0473, Val Loss: 0.0461, Val RMSE: 0.2146 Val R2: 0.2372\n",
      "Epoch: 277, Train Loss: 0.0475, Val Loss: 0.0458, Val RMSE: 0.2141 Val R2: 0.2409\n",
      "Epoch: 278, Train Loss: 0.0474, Val Loss: 0.0459, Val RMSE: 0.2142 Val R2: 0.2398\n",
      "Epoch: 279, Train Loss: 0.0473, Val Loss: 0.0460, Val RMSE: 0.2146 Val R2: 0.2375\n",
      "Epoch: 280, Train Loss: 0.0472, Val Loss: 0.0458, Val RMSE: 0.2141 Val R2: 0.2411\n",
      "Epoch: 281, Train Loss: 0.0474, Val Loss: 0.0459, Val RMSE: 0.2143 Val R2: 0.2391\n",
      "Epoch: 282, Train Loss: 0.0472, Val Loss: 0.0458, Val RMSE: 0.2140 Val R2: 0.2418\n",
      "Epoch: 283, Train Loss: 0.0474, Val Loss: 0.0458, Val RMSE: 0.2140 Val R2: 0.2415\n",
      "Epoch: 284, Train Loss: 0.0473, Val Loss: 0.0460, Val RMSE: 0.2145 Val R2: 0.2383\n",
      "Epoch: 285, Train Loss: 0.0473, Val Loss: 0.0458, Val RMSE: 0.2141 Val R2: 0.2412\n",
      "Epoch: 286, Train Loss: 0.0473, Val Loss: 0.0459, Val RMSE: 0.2143 Val R2: 0.2396\n",
      "Epoch: 287, Train Loss: 0.0473, Val Loss: 0.0457, Val RMSE: 0.2138 Val R2: 0.2427\n",
      "Epoch: 288, Train Loss: 0.0473, Val Loss: 0.0458, Val RMSE: 0.2141 Val R2: 0.2408\n",
      "Epoch: 289, Train Loss: 0.0473, Val Loss: 0.0460, Val RMSE: 0.2144 Val R2: 0.2387\n",
      "Epoch: 290, Train Loss: 0.0472, Val Loss: 0.0458, Val RMSE: 0.2140 Val R2: 0.2414\n",
      "Epoch: 291, Train Loss: 0.0473, Val Loss: 0.0461, Val RMSE: 0.2146 Val R2: 0.2372\n",
      "Epoch: 292, Train Loss: 0.0472, Val Loss: 0.0457, Val RMSE: 0.2138 Val R2: 0.2428\n",
      "Epoch: 293, Train Loss: 0.0474, Val Loss: 0.0459, Val RMSE: 0.2141 Val R2: 0.2406\n",
      "Epoch: 294, Train Loss: 0.0473, Val Loss: 0.0459, Val RMSE: 0.2141 Val R2: 0.2406\n",
      "Epoch: 295, Train Loss: 0.0472, Val Loss: 0.0458, Val RMSE: 0.2140 Val R2: 0.2416\n",
      "Epoch: 296, Train Loss: 0.0473, Val Loss: 0.0460, Val RMSE: 0.2145 Val R2: 0.2380\n",
      "Epoch: 297, Train Loss: 0.0472, Val Loss: 0.0457, Val RMSE: 0.2137 Val R2: 0.2436\n",
      "Epoch: 298, Train Loss: 0.0473, Val Loss: 0.0458, Val RMSE: 0.2140 Val R2: 0.2413\n",
      "Epoch: 299, Train Loss: 0.0472, Val Loss: 0.0459, Val RMSE: 0.2142 Val R2: 0.2404\n",
      "Epoch: 300, Train Loss: 0.0472, Val Loss: 0.0457, Val RMSE: 0.2138 Val R2: 0.2427\n",
      "Test RMSE: 0.2137, Test MAE: 0.1800, Test R2: 0.2404\n"
     ]
    }
   ],
   "source": [
    "in_features = data.x.shape[1]\n",
    "hidden_features = 16\n",
    "out_features = 1\n",
    "num_layers = 2\n",
    "epochs = 300\n",
    "lr = 0.01\n",
    "\n",
    "model = GraphSageRegression(in_features, hidden_features, out_features, num_layers)\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "def train_model(model, data, mask):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index)[mask]\n",
    "    loss = criterion(out, data.y[mask].view(-1, 1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "def evaluate_model(model, data, mask):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data.x, data.edge_index)[mask]\n",
    "        mse = criterion(out, data.y[mask].view(-1, 1))\n",
    "        r2 = r2_score(data.y[mask].view(-1, 1), out)\n",
    "        mae = mean_absolute_error(out, data.y[mask].view(-1, 1))\n",
    "        rmse = torch.sqrt(mse)\n",
    "    return mse.item(), rmse, mae, r2\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_maes = []\n",
    "val_maes = []\n",
    "val_rmses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train_model(model, data, data.train_mask)\n",
    "    val_mse, val_rmse, val_mae, r2 = evaluate_model(model, data, data.val_mask)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_mse)\n",
    "    val_maes.append(val_mae)\n",
    "    val_rmses.append(val_rmse)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:03d}, Train Loss: {train_loss:.4f}, Val Loss: {val_mse:.4f}, Val RMSE: {val_rmse:.4f} Val R2: {r2:.4f}')\n",
    "\n",
    "test_mse, test_rmse, test_mae, test_r2 = evaluate_model(model, data, data.test_mask)\n",
    "print(f'Test RMSE: {test_rmse:.4f}, Test MAE: {test_mae:.4f}, Test R2: {test_r2:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Loss')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAT Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Train Loss: 0.5561, Val Loss: 1.8209, Val RMSE: 1.3494 Val R2: -29.1580\n",
      "Epoch: 002, Train Loss: 2.5423, Val Loss: 0.1837, Val RMSE: 0.4286 Val R2: -2.0427\n",
      "Epoch: 003, Train Loss: 0.3671, Val Loss: 0.2652, Val RMSE: 0.5150 Val R2: -3.3923\n",
      "Epoch: 004, Train Loss: 0.4221, Val Loss: 0.5988, Val RMSE: 0.7739 Val R2: -8.9179\n",
      "Epoch: 005, Train Loss: 0.7825, Val Loss: 0.4563, Val RMSE: 0.6755 Val R2: -6.5571\n",
      "Epoch: 006, Train Loss: 0.5895, Val Loss: 0.2081, Val RMSE: 0.4562 Val R2: -2.4470\n",
      "Epoch: 007, Train Loss: 0.2950, Val Loss: 0.0837, Val RMSE: 0.2894 Val R2: -0.3867\n",
      "Epoch: 008, Train Loss: 0.1507, Val Loss: 0.0935, Val RMSE: 0.3057 Val R2: -0.5482\n",
      "Epoch: 009, Train Loss: 0.1569, Val Loss: 0.1515, Val RMSE: 0.3892 Val R2: -1.5092\n",
      "Epoch: 010, Train Loss: 0.2212, Val Loss: 0.1744, Val RMSE: 0.4176 Val R2: -1.8881\n",
      "Epoch: 011, Train Loss: 0.2511, Val Loss: 0.1467, Val RMSE: 0.3830 Val R2: -1.4290\n",
      "Epoch: 012, Train Loss: 0.2232, Val Loss: 0.1012, Val RMSE: 0.3181 Val R2: -0.6761\n",
      "Epoch: 013, Train Loss: 0.1666, Val Loss: 0.0712, Val RMSE: 0.2668 Val R2: -0.1790\n",
      "Epoch: 014, Train Loss: 0.1308, Val Loss: 0.0676, Val RMSE: 0.2600 Val R2: -0.1196\n",
      "Epoch: 015, Train Loss: 0.1210, Val Loss: 0.0813, Val RMSE: 0.2852 Val R2: -0.3470\n",
      "Epoch: 016, Train Loss: 0.1280, Val Loss: 0.0957, Val RMSE: 0.3094 Val R2: -0.5856\n",
      "Epoch: 017, Train Loss: 0.1366, Val Loss: 0.1006, Val RMSE: 0.3172 Val R2: -0.6664\n",
      "Epoch: 018, Train Loss: 0.1376, Val Loss: 0.0959, Val RMSE: 0.3097 Val R2: -0.5882\n",
      "Epoch: 019, Train Loss: 0.1312, Val Loss: 0.0867, Val RMSE: 0.2944 Val R2: -0.4356\n",
      "Epoch: 020, Train Loss: 0.1215, Val Loss: 0.0777, Val RMSE: 0.2787 Val R2: -0.2864\n",
      "Epoch: 021, Train Loss: 0.1116, Val Loss: 0.0708, Val RMSE: 0.2661 Val R2: -0.1725\n",
      "Epoch: 022, Train Loss: 0.1043, Val Loss: 0.0659, Val RMSE: 0.2567 Val R2: -0.0915\n",
      "Epoch: 023, Train Loss: 0.0983, Val Loss: 0.0623, Val RMSE: 0.2496 Val R2: -0.0322\n",
      "Epoch: 024, Train Loss: 0.0926, Val Loss: 0.0598, Val RMSE: 0.2446 Val R2: 0.0095\n",
      "Epoch: 025, Train Loss: 0.0897, Val Loss: 0.0583, Val RMSE: 0.2415 Val R2: 0.0342\n",
      "Epoch: 026, Train Loss: 0.0869, Val Loss: 0.0579, Val RMSE: 0.2406 Val R2: 0.0414\n",
      "Epoch: 027, Train Loss: 0.0873, Val Loss: 0.0582, Val RMSE: 0.2412 Val R2: 0.0363\n",
      "Epoch: 028, Train Loss: 0.0884, Val Loss: 0.0586, Val RMSE: 0.2420 Val R2: 0.0300\n",
      "Epoch: 029, Train Loss: 0.0890, Val Loss: 0.0584, Val RMSE: 0.2417 Val R2: 0.0327\n",
      "Epoch: 030, Train Loss: 0.0865, Val Loss: 0.0575, Val RMSE: 0.2398 Val R2: 0.0480\n",
      "Epoch: 031, Train Loss: 0.0844, Val Loss: 0.0560, Val RMSE: 0.2367 Val R2: 0.0718\n",
      "Epoch: 032, Train Loss: 0.0807, Val Loss: 0.0546, Val RMSE: 0.2336 Val R2: 0.0963\n",
      "Epoch: 033, Train Loss: 0.0754, Val Loss: 0.0536, Val RMSE: 0.2315 Val R2: 0.1121\n",
      "Epoch: 034, Train Loss: 0.0726, Val Loss: 0.0535, Val RMSE: 0.2313 Val R2: 0.1143\n",
      "Epoch: 035, Train Loss: 0.0707, Val Loss: 0.0542, Val RMSE: 0.2327 Val R2: 0.1029\n",
      "Epoch: 036, Train Loss: 0.0699, Val Loss: 0.0554, Val RMSE: 0.2353 Val R2: 0.0829\n",
      "Epoch: 037, Train Loss: 0.0707, Val Loss: 0.0566, Val RMSE: 0.2380 Val R2: 0.0619\n",
      "Epoch: 038, Train Loss: 0.0714, Val Loss: 0.0576, Val RMSE: 0.2400 Val R2: 0.0463\n",
      "Epoch: 039, Train Loss: 0.0719, Val Loss: 0.0579, Val RMSE: 0.2407 Val R2: 0.0406\n",
      "Epoch: 040, Train Loss: 0.0712, Val Loss: 0.0577, Val RMSE: 0.2401 Val R2: 0.0451\n",
      "Epoch: 041, Train Loss: 0.0708, Val Loss: 0.0569, Val RMSE: 0.2386 Val R2: 0.0571\n",
      "Epoch: 042, Train Loss: 0.0690, Val Loss: 0.0560, Val RMSE: 0.2366 Val R2: 0.0728\n",
      "Epoch: 043, Train Loss: 0.0681, Val Loss: 0.0550, Val RMSE: 0.2346 Val R2: 0.0884\n",
      "Epoch: 044, Train Loss: 0.0659, Val Loss: 0.0543, Val RMSE: 0.2329 Val R2: 0.1014\n",
      "Epoch: 045, Train Loss: 0.0648, Val Loss: 0.0537, Val RMSE: 0.2317 Val R2: 0.1110\n",
      "Epoch: 046, Train Loss: 0.0644, Val Loss: 0.0533, Val RMSE: 0.2308 Val R2: 0.1175\n",
      "Epoch: 047, Train Loss: 0.0636, Val Loss: 0.0530, Val RMSE: 0.2302 Val R2: 0.1223\n",
      "Epoch: 048, Train Loss: 0.0636, Val Loss: 0.0528, Val RMSE: 0.2297 Val R2: 0.1262\n",
      "Epoch: 049, Train Loss: 0.0633, Val Loss: 0.0525, Val RMSE: 0.2292 Val R2: 0.1300\n",
      "Epoch: 050, Train Loss: 0.0631, Val Loss: 0.0523, Val RMSE: 0.2287 Val R2: 0.1334\n",
      "Epoch: 051, Train Loss: 0.0627, Val Loss: 0.0522, Val RMSE: 0.2284 Val R2: 0.1362\n",
      "Epoch: 052, Train Loss: 0.0623, Val Loss: 0.0521, Val RMSE: 0.2281 Val R2: 0.1379\n",
      "Epoch: 053, Train Loss: 0.0620, Val Loss: 0.0520, Val RMSE: 0.2281 Val R2: 0.1385\n",
      "Epoch: 054, Train Loss: 0.0618, Val Loss: 0.0520, Val RMSE: 0.2281 Val R2: 0.1382\n",
      "Epoch: 055, Train Loss: 0.0611, Val Loss: 0.0521, Val RMSE: 0.2282 Val R2: 0.1372\n",
      "Epoch: 056, Train Loss: 0.0608, Val Loss: 0.0522, Val RMSE: 0.2284 Val R2: 0.1360\n",
      "Epoch: 057, Train Loss: 0.0609, Val Loss: 0.0522, Val RMSE: 0.2286 Val R2: 0.1348\n",
      "Epoch: 058, Train Loss: 0.0604, Val Loss: 0.0523, Val RMSE: 0.2287 Val R2: 0.1336\n",
      "Epoch: 059, Train Loss: 0.0600, Val Loss: 0.0524, Val RMSE: 0.2289 Val R2: 0.1323\n",
      "Epoch: 060, Train Loss: 0.0598, Val Loss: 0.0525, Val RMSE: 0.2291 Val R2: 0.1307\n",
      "Epoch: 061, Train Loss: 0.0596, Val Loss: 0.0526, Val RMSE: 0.2294 Val R2: 0.1287\n",
      "Epoch: 062, Train Loss: 0.0592, Val Loss: 0.0527, Val RMSE: 0.2297 Val R2: 0.1265\n",
      "Epoch: 063, Train Loss: 0.0591, Val Loss: 0.0529, Val RMSE: 0.2299 Val R2: 0.1245\n",
      "Epoch: 064, Train Loss: 0.0591, Val Loss: 0.0530, Val RMSE: 0.2301 Val R2: 0.1229\n",
      "Epoch: 065, Train Loss: 0.0592, Val Loss: 0.0530, Val RMSE: 0.2302 Val R2: 0.1221\n",
      "Epoch: 066, Train Loss: 0.0590, Val Loss: 0.0530, Val RMSE: 0.2302 Val R2: 0.1222\n",
      "Epoch: 067, Train Loss: 0.0589, Val Loss: 0.0529, Val RMSE: 0.2301 Val R2: 0.1234\n",
      "Epoch: 068, Train Loss: 0.0587, Val Loss: 0.0528, Val RMSE: 0.2298 Val R2: 0.1253\n",
      "Epoch: 069, Train Loss: 0.0584, Val Loss: 0.0527, Val RMSE: 0.2295 Val R2: 0.1277\n",
      "Epoch: 070, Train Loss: 0.0584, Val Loss: 0.0525, Val RMSE: 0.2291 Val R2: 0.1304\n",
      "Epoch: 071, Train Loss: 0.0585, Val Loss: 0.0524, Val RMSE: 0.2288 Val R2: 0.1328\n",
      "Epoch: 072, Train Loss: 0.0581, Val Loss: 0.0522, Val RMSE: 0.2286 Val R2: 0.1348\n",
      "Epoch: 073, Train Loss: 0.0579, Val Loss: 0.0521, Val RMSE: 0.2283 Val R2: 0.1364\n",
      "Epoch: 074, Train Loss: 0.0580, Val Loss: 0.0521, Val RMSE: 0.2282 Val R2: 0.1377\n",
      "Epoch: 075, Train Loss: 0.0579, Val Loss: 0.0520, Val RMSE: 0.2281 Val R2: 0.1386\n",
      "Epoch: 076, Train Loss: 0.0576, Val Loss: 0.0520, Val RMSE: 0.2280 Val R2: 0.1392\n",
      "Epoch: 077, Train Loss: 0.0577, Val Loss: 0.0520, Val RMSE: 0.2279 Val R2: 0.1396\n",
      "Epoch: 078, Train Loss: 0.0576, Val Loss: 0.0519, Val RMSE: 0.2279 Val R2: 0.1398\n",
      "Epoch: 079, Train Loss: 0.0574, Val Loss: 0.0519, Val RMSE: 0.2279 Val R2: 0.1398\n",
      "Epoch: 080, Train Loss: 0.0576, Val Loss: 0.0520, Val RMSE: 0.2279 Val R2: 0.1396\n",
      "Epoch: 081, Train Loss: 0.0573, Val Loss: 0.0520, Val RMSE: 0.2280 Val R2: 0.1391\n",
      "Epoch: 082, Train Loss: 0.0574, Val Loss: 0.0520, Val RMSE: 0.2281 Val R2: 0.1385\n",
      "Epoch: 083, Train Loss: 0.0573, Val Loss: 0.0521, Val RMSE: 0.2282 Val R2: 0.1376\n",
      "Epoch: 084, Train Loss: 0.0573, Val Loss: 0.0521, Val RMSE: 0.2283 Val R2: 0.1367\n",
      "Epoch: 085, Train Loss: 0.0572, Val Loss: 0.0522, Val RMSE: 0.2284 Val R2: 0.1358\n",
      "Epoch: 086, Train Loss: 0.0575, Val Loss: 0.0522, Val RMSE: 0.2286 Val R2: 0.1348\n",
      "Epoch: 087, Train Loss: 0.0571, Val Loss: 0.0523, Val RMSE: 0.2287 Val R2: 0.1341\n",
      "Epoch: 088, Train Loss: 0.0569, Val Loss: 0.0523, Val RMSE: 0.2287 Val R2: 0.1336\n",
      "Epoch: 089, Train Loss: 0.0569, Val Loss: 0.0523, Val RMSE: 0.2288 Val R2: 0.1333\n",
      "Epoch: 090, Train Loss: 0.0571, Val Loss: 0.0523, Val RMSE: 0.2288 Val R2: 0.1331\n",
      "Epoch: 091, Train Loss: 0.0568, Val Loss: 0.0523, Val RMSE: 0.2288 Val R2: 0.1332\n",
      "Epoch: 092, Train Loss: 0.0569, Val Loss: 0.0523, Val RMSE: 0.2287 Val R2: 0.1335\n",
      "Epoch: 093, Train Loss: 0.0568, Val Loss: 0.0523, Val RMSE: 0.2287 Val R2: 0.1339\n",
      "Epoch: 094, Train Loss: 0.0568, Val Loss: 0.0523, Val RMSE: 0.2286 Val R2: 0.1344\n",
      "Epoch: 095, Train Loss: 0.0567, Val Loss: 0.0522, Val RMSE: 0.2285 Val R2: 0.1351\n",
      "Epoch: 096, Train Loss: 0.0566, Val Loss: 0.0522, Val RMSE: 0.2284 Val R2: 0.1358\n",
      "Epoch: 097, Train Loss: 0.0571, Val Loss: 0.0521, Val RMSE: 0.2283 Val R2: 0.1364\n",
      "Epoch: 098, Train Loss: 0.0567, Val Loss: 0.0521, Val RMSE: 0.2283 Val R2: 0.1370\n",
      "Epoch: 099, Train Loss: 0.0566, Val Loss: 0.0521, Val RMSE: 0.2282 Val R2: 0.1375\n",
      "Epoch: 100, Train Loss: 0.0565, Val Loss: 0.0520, Val RMSE: 0.2281 Val R2: 0.1380\n",
      "Epoch: 101, Train Loss: 0.0565, Val Loss: 0.0520, Val RMSE: 0.2281 Val R2: 0.1384\n",
      "Epoch: 102, Train Loss: 0.0566, Val Loss: 0.0520, Val RMSE: 0.2280 Val R2: 0.1387\n",
      "Epoch: 103, Train Loss: 0.0565, Val Loss: 0.0520, Val RMSE: 0.2280 Val R2: 0.1388\n",
      "Epoch: 104, Train Loss: 0.0564, Val Loss: 0.0520, Val RMSE: 0.2280 Val R2: 0.1389\n",
      "Epoch: 105, Train Loss: 0.0563, Val Loss: 0.0520, Val RMSE: 0.2280 Val R2: 0.1389\n",
      "Epoch: 106, Train Loss: 0.0563, Val Loss: 0.0520, Val RMSE: 0.2280 Val R2: 0.1388\n",
      "Epoch: 107, Train Loss: 0.0564, Val Loss: 0.0520, Val RMSE: 0.2280 Val R2: 0.1387\n",
      "Epoch: 108, Train Loss: 0.0562, Val Loss: 0.0520, Val RMSE: 0.2281 Val R2: 0.1386\n",
      "Epoch: 109, Train Loss: 0.0562, Val Loss: 0.0520, Val RMSE: 0.2281 Val R2: 0.1384\n",
      "Epoch: 110, Train Loss: 0.0562, Val Loss: 0.0520, Val RMSE: 0.2281 Val R2: 0.1382\n",
      "Epoch: 111, Train Loss: 0.0562, Val Loss: 0.0520, Val RMSE: 0.2281 Val R2: 0.1380\n",
      "Epoch: 112, Train Loss: 0.0562, Val Loss: 0.0521, Val RMSE: 0.2282 Val R2: 0.1378\n",
      "Epoch: 113, Train Loss: 0.0560, Val Loss: 0.0521, Val RMSE: 0.2282 Val R2: 0.1377\n",
      "Epoch: 114, Train Loss: 0.0560, Val Loss: 0.0521, Val RMSE: 0.2282 Val R2: 0.1376\n",
      "Epoch: 115, Train Loss: 0.0564, Val Loss: 0.0521, Val RMSE: 0.2282 Val R2: 0.1375\n",
      "Epoch: 116, Train Loss: 0.0561, Val Loss: 0.0521, Val RMSE: 0.2282 Val R2: 0.1374\n",
      "Epoch: 117, Train Loss: 0.0560, Val Loss: 0.0521, Val RMSE: 0.2282 Val R2: 0.1373\n",
      "Epoch: 118, Train Loss: 0.0560, Val Loss: 0.0521, Val RMSE: 0.2282 Val R2: 0.1373\n",
      "Epoch: 119, Train Loss: 0.0560, Val Loss: 0.0521, Val RMSE: 0.2282 Val R2: 0.1373\n",
      "Epoch: 120, Train Loss: 0.0559, Val Loss: 0.0521, Val RMSE: 0.2282 Val R2: 0.1374\n",
      "Epoch: 121, Train Loss: 0.0558, Val Loss: 0.0521, Val RMSE: 0.2282 Val R2: 0.1377\n",
      "Epoch: 122, Train Loss: 0.0559, Val Loss: 0.0521, Val RMSE: 0.2282 Val R2: 0.1379\n",
      "Epoch: 123, Train Loss: 0.0559, Val Loss: 0.0520, Val RMSE: 0.2281 Val R2: 0.1382\n",
      "Epoch: 124, Train Loss: 0.0559, Val Loss: 0.0520, Val RMSE: 0.2281 Val R2: 0.1385\n",
      "Epoch: 125, Train Loss: 0.0559, Val Loss: 0.0520, Val RMSE: 0.2280 Val R2: 0.1388\n",
      "Epoch: 126, Train Loss: 0.0560, Val Loss: 0.0520, Val RMSE: 0.2280 Val R2: 0.1390\n",
      "Epoch: 127, Train Loss: 0.0558, Val Loss: 0.0520, Val RMSE: 0.2280 Val R2: 0.1393\n",
      "Epoch: 128, Train Loss: 0.0557, Val Loss: 0.0520, Val RMSE: 0.2280 Val R2: 0.1394\n",
      "Epoch: 129, Train Loss: 0.0559, Val Loss: 0.0520, Val RMSE: 0.2279 Val R2: 0.1395\n",
      "Epoch: 130, Train Loss: 0.0556, Val Loss: 0.0519, Val RMSE: 0.2279 Val R2: 0.1396\n",
      "Epoch: 131, Train Loss: 0.0559, Val Loss: 0.0519, Val RMSE: 0.2279 Val R2: 0.1396\n",
      "Epoch: 132, Train Loss: 0.0557, Val Loss: 0.0520, Val RMSE: 0.2279 Val R2: 0.1396\n",
      "Epoch: 133, Train Loss: 0.0557, Val Loss: 0.0520, Val RMSE: 0.2279 Val R2: 0.1395\n",
      "Epoch: 134, Train Loss: 0.0561, Val Loss: 0.0520, Val RMSE: 0.2280 Val R2: 0.1393\n",
      "Epoch: 135, Train Loss: 0.0556, Val Loss: 0.0520, Val RMSE: 0.2280 Val R2: 0.1391\n",
      "Epoch: 136, Train Loss: 0.0557, Val Loss: 0.0520, Val RMSE: 0.2280 Val R2: 0.1390\n",
      "Epoch: 137, Train Loss: 0.0557, Val Loss: 0.0520, Val RMSE: 0.2280 Val R2: 0.1388\n",
      "Epoch: 138, Train Loss: 0.0556, Val Loss: 0.0520, Val RMSE: 0.2281 Val R2: 0.1386\n",
      "Epoch: 139, Train Loss: 0.0555, Val Loss: 0.0520, Val RMSE: 0.2281 Val R2: 0.1384\n",
      "Epoch: 140, Train Loss: 0.0555, Val Loss: 0.0520, Val RMSE: 0.2281 Val R2: 0.1383\n",
      "Epoch: 141, Train Loss: 0.0555, Val Loss: 0.0520, Val RMSE: 0.2281 Val R2: 0.1383\n",
      "Epoch: 142, Train Loss: 0.0557, Val Loss: 0.0520, Val RMSE: 0.2281 Val R2: 0.1383\n",
      "Epoch: 143, Train Loss: 0.0558, Val Loss: 0.0520, Val RMSE: 0.2281 Val R2: 0.1383\n",
      "Epoch: 144, Train Loss: 0.0554, Val Loss: 0.0520, Val RMSE: 0.2281 Val R2: 0.1385\n",
      "Epoch: 145, Train Loss: 0.0554, Val Loss: 0.0520, Val RMSE: 0.2280 Val R2: 0.1387\n",
      "Epoch: 146, Train Loss: 0.0554, Val Loss: 0.0520, Val RMSE: 0.2280 Val R2: 0.1389\n",
      "Epoch: 147, Train Loss: 0.0554, Val Loss: 0.0520, Val RMSE: 0.2280 Val R2: 0.1391\n",
      "Epoch: 148, Train Loss: 0.0556, Val Loss: 0.0520, Val RMSE: 0.2280 Val R2: 0.1393\n",
      "Epoch: 149, Train Loss: 0.0553, Val Loss: 0.0520, Val RMSE: 0.2279 Val R2: 0.1394\n",
      "Epoch: 150, Train Loss: 0.0554, Val Loss: 0.0520, Val RMSE: 0.2279 Val R2: 0.1396\n",
      "Epoch: 151, Train Loss: 0.0554, Val Loss: 0.0519, Val RMSE: 0.2279 Val R2: 0.1397\n",
      "Epoch: 152, Train Loss: 0.0553, Val Loss: 0.0519, Val RMSE: 0.2279 Val R2: 0.1399\n",
      "Epoch: 153, Train Loss: 0.0553, Val Loss: 0.0519, Val RMSE: 0.2279 Val R2: 0.1400\n",
      "Epoch: 154, Train Loss: 0.0554, Val Loss: 0.0519, Val RMSE: 0.2279 Val R2: 0.1400\n",
      "Epoch: 155, Train Loss: 0.0552, Val Loss: 0.0519, Val RMSE: 0.2279 Val R2: 0.1400\n",
      "Epoch: 156, Train Loss: 0.0554, Val Loss: 0.0519, Val RMSE: 0.2279 Val R2: 0.1400\n",
      "Epoch: 157, Train Loss: 0.0552, Val Loss: 0.0519, Val RMSE: 0.2279 Val R2: 0.1400\n",
      "Epoch: 158, Train Loss: 0.0552, Val Loss: 0.0519, Val RMSE: 0.2279 Val R2: 0.1400\n",
      "Epoch: 159, Train Loss: 0.0553, Val Loss: 0.0519, Val RMSE: 0.2279 Val R2: 0.1399\n",
      "Epoch: 160, Train Loss: 0.0552, Val Loss: 0.0519, Val RMSE: 0.2279 Val R2: 0.1399\n",
      "Epoch: 161, Train Loss: 0.0553, Val Loss: 0.0519, Val RMSE: 0.2279 Val R2: 0.1399\n",
      "Epoch: 162, Train Loss: 0.0551, Val Loss: 0.0519, Val RMSE: 0.2279 Val R2: 0.1399\n",
      "Epoch: 163, Train Loss: 0.0552, Val Loss: 0.0519, Val RMSE: 0.2279 Val R2: 0.1400\n",
      "Epoch: 164, Train Loss: 0.0551, Val Loss: 0.0519, Val RMSE: 0.2279 Val R2: 0.1401\n",
      "Epoch: 165, Train Loss: 0.0552, Val Loss: 0.0519, Val RMSE: 0.2278 Val R2: 0.1402\n",
      "Epoch: 166, Train Loss: 0.0552, Val Loss: 0.0519, Val RMSE: 0.2278 Val R2: 0.1403\n",
      "Epoch: 167, Train Loss: 0.0553, Val Loss: 0.0519, Val RMSE: 0.2278 Val R2: 0.1403\n",
      "Epoch: 168, Train Loss: 0.0550, Val Loss: 0.0519, Val RMSE: 0.2278 Val R2: 0.1403\n",
      "Epoch: 169, Train Loss: 0.0551, Val Loss: 0.0519, Val RMSE: 0.2278 Val R2: 0.1404\n",
      "Epoch: 170, Train Loss: 0.0551, Val Loss: 0.0519, Val RMSE: 0.2278 Val R2: 0.1404\n",
      "Epoch: 171, Train Loss: 0.0551, Val Loss: 0.0519, Val RMSE: 0.2278 Val R2: 0.1404\n",
      "Epoch: 172, Train Loss: 0.0550, Val Loss: 0.0519, Val RMSE: 0.2278 Val R2: 0.1405\n",
      "Epoch: 173, Train Loss: 0.0551, Val Loss: 0.0519, Val RMSE: 0.2278 Val R2: 0.1406\n",
      "Epoch: 174, Train Loss: 0.0552, Val Loss: 0.0519, Val RMSE: 0.2278 Val R2: 0.1406\n",
      "Epoch: 175, Train Loss: 0.0551, Val Loss: 0.0519, Val RMSE: 0.2278 Val R2: 0.1407\n",
      "Epoch: 176, Train Loss: 0.0549, Val Loss: 0.0519, Val RMSE: 0.2278 Val R2: 0.1407\n",
      "Epoch: 177, Train Loss: 0.0550, Val Loss: 0.0519, Val RMSE: 0.2278 Val R2: 0.1407\n",
      "Epoch: 178, Train Loss: 0.0550, Val Loss: 0.0519, Val RMSE: 0.2278 Val R2: 0.1408\n",
      "Epoch: 179, Train Loss: 0.0553, Val Loss: 0.0519, Val RMSE: 0.2278 Val R2: 0.1407\n",
      "Epoch: 180, Train Loss: 0.0550, Val Loss: 0.0519, Val RMSE: 0.2278 Val R2: 0.1407\n",
      "Epoch: 181, Train Loss: 0.0549, Val Loss: 0.0519, Val RMSE: 0.2278 Val R2: 0.1407\n",
      "Epoch: 182, Train Loss: 0.0550, Val Loss: 0.0519, Val RMSE: 0.2278 Val R2: 0.1407\n",
      "Epoch: 183, Train Loss: 0.0551, Val Loss: 0.0519, Val RMSE: 0.2278 Val R2: 0.1407\n",
      "Epoch: 184, Train Loss: 0.0549, Val Loss: 0.0519, Val RMSE: 0.2278 Val R2: 0.1407\n",
      "Epoch: 185, Train Loss: 0.0550, Val Loss: 0.0519, Val RMSE: 0.2278 Val R2: 0.1407\n",
      "Epoch: 186, Train Loss: 0.0550, Val Loss: 0.0519, Val RMSE: 0.2278 Val R2: 0.1408\n",
      "Epoch: 187, Train Loss: 0.0549, Val Loss: 0.0519, Val RMSE: 0.2278 Val R2: 0.1409\n",
      "Epoch: 188, Train Loss: 0.0551, Val Loss: 0.0519, Val RMSE: 0.2278 Val R2: 0.1409\n",
      "Epoch: 189, Train Loss: 0.0549, Val Loss: 0.0519, Val RMSE: 0.2278 Val R2: 0.1409\n",
      "Epoch: 190, Train Loss: 0.0549, Val Loss: 0.0519, Val RMSE: 0.2278 Val R2: 0.1409\n",
      "Epoch: 191, Train Loss: 0.0550, Val Loss: 0.0519, Val RMSE: 0.2278 Val R2: 0.1409\n",
      "Epoch: 192, Train Loss: 0.0550, Val Loss: 0.0519, Val RMSE: 0.2278 Val R2: 0.1409\n",
      "Epoch: 193, Train Loss: 0.0548, Val Loss: 0.0519, Val RMSE: 0.2278 Val R2: 0.1408\n",
      "Epoch: 194, Train Loss: 0.0549, Val Loss: 0.0519, Val RMSE: 0.2278 Val R2: 0.1408\n",
      "Epoch: 195, Train Loss: 0.0548, Val Loss: 0.0519, Val RMSE: 0.2278 Val R2: 0.1409\n",
      "Epoch: 196, Train Loss: 0.0548, Val Loss: 0.0519, Val RMSE: 0.2277 Val R2: 0.1411\n",
      "Epoch: 197, Train Loss: 0.0548, Val Loss: 0.0518, Val RMSE: 0.2277 Val R2: 0.1413\n",
      "Epoch: 198, Train Loss: 0.0548, Val Loss: 0.0518, Val RMSE: 0.2277 Val R2: 0.1415\n",
      "Epoch: 199, Train Loss: 0.0548, Val Loss: 0.0518, Val RMSE: 0.2276 Val R2: 0.1417\n",
      "Epoch: 200, Train Loss: 0.0549, Val Loss: 0.0518, Val RMSE: 0.2276 Val R2: 0.1419\n",
      "Epoch: 201, Train Loss: 0.0547, Val Loss: 0.0518, Val RMSE: 0.2276 Val R2: 0.1421\n",
      "Epoch: 202, Train Loss: 0.0548, Val Loss: 0.0518, Val RMSE: 0.2276 Val R2: 0.1422\n",
      "Epoch: 203, Train Loss: 0.0549, Val Loss: 0.0518, Val RMSE: 0.2276 Val R2: 0.1422\n",
      "Epoch: 204, Train Loss: 0.0546, Val Loss: 0.0518, Val RMSE: 0.2276 Val R2: 0.1422\n",
      "Epoch: 205, Train Loss: 0.0548, Val Loss: 0.0518, Val RMSE: 0.2276 Val R2: 0.1421\n",
      "Epoch: 206, Train Loss: 0.0549, Val Loss: 0.0518, Val RMSE: 0.2276 Val R2: 0.1420\n",
      "Epoch: 207, Train Loss: 0.0549, Val Loss: 0.0518, Val RMSE: 0.2276 Val R2: 0.1418\n",
      "Epoch: 208, Train Loss: 0.0548, Val Loss: 0.0518, Val RMSE: 0.2276 Val R2: 0.1417\n",
      "Epoch: 209, Train Loss: 0.0547, Val Loss: 0.0518, Val RMSE: 0.2276 Val R2: 0.1417\n",
      "Epoch: 210, Train Loss: 0.0547, Val Loss: 0.0518, Val RMSE: 0.2276 Val R2: 0.1417\n",
      "Epoch: 211, Train Loss: 0.0546, Val Loss: 0.0518, Val RMSE: 0.2276 Val R2: 0.1418\n",
      "Epoch: 212, Train Loss: 0.0547, Val Loss: 0.0518, Val RMSE: 0.2276 Val R2: 0.1418\n",
      "Epoch: 213, Train Loss: 0.0547, Val Loss: 0.0518, Val RMSE: 0.2276 Val R2: 0.1419\n",
      "Epoch: 214, Train Loss: 0.0545, Val Loss: 0.0518, Val RMSE: 0.2276 Val R2: 0.1421\n",
      "Epoch: 215, Train Loss: 0.0546, Val Loss: 0.0518, Val RMSE: 0.2276 Val R2: 0.1422\n",
      "Epoch: 216, Train Loss: 0.0547, Val Loss: 0.0518, Val RMSE: 0.2276 Val R2: 0.1423\n",
      "Epoch: 217, Train Loss: 0.0548, Val Loss: 0.0518, Val RMSE: 0.2276 Val R2: 0.1424\n",
      "Epoch: 218, Train Loss: 0.0546, Val Loss: 0.0518, Val RMSE: 0.2275 Val R2: 0.1425\n",
      "Epoch: 219, Train Loss: 0.0546, Val Loss: 0.0518, Val RMSE: 0.2275 Val R2: 0.1425\n",
      "Epoch: 220, Train Loss: 0.0546, Val Loss: 0.0518, Val RMSE: 0.2275 Val R2: 0.1426\n",
      "Epoch: 221, Train Loss: 0.0547, Val Loss: 0.0518, Val RMSE: 0.2275 Val R2: 0.1426\n",
      "Epoch: 222, Train Loss: 0.0547, Val Loss: 0.0518, Val RMSE: 0.2276 Val R2: 0.1424\n",
      "Epoch: 223, Train Loss: 0.0545, Val Loss: 0.0518, Val RMSE: 0.2276 Val R2: 0.1423\n",
      "Epoch: 224, Train Loss: 0.0546, Val Loss: 0.0518, Val RMSE: 0.2276 Val R2: 0.1422\n",
      "Epoch: 225, Train Loss: 0.0545, Val Loss: 0.0518, Val RMSE: 0.2276 Val R2: 0.1422\n",
      "Epoch: 226, Train Loss: 0.0546, Val Loss: 0.0518, Val RMSE: 0.2276 Val R2: 0.1422\n",
      "Epoch: 227, Train Loss: 0.0546, Val Loss: 0.0518, Val RMSE: 0.2276 Val R2: 0.1423\n",
      "Epoch: 228, Train Loss: 0.0547, Val Loss: 0.0518, Val RMSE: 0.2276 Val R2: 0.1424\n",
      "Epoch: 229, Train Loss: 0.0545, Val Loss: 0.0518, Val RMSE: 0.2275 Val R2: 0.1426\n",
      "Epoch: 230, Train Loss: 0.0545, Val Loss: 0.0518, Val RMSE: 0.2275 Val R2: 0.1428\n",
      "Epoch: 231, Train Loss: 0.0545, Val Loss: 0.0517, Val RMSE: 0.2275 Val R2: 0.1431\n",
      "Epoch: 232, Train Loss: 0.0545, Val Loss: 0.0517, Val RMSE: 0.2274 Val R2: 0.1433\n",
      "Epoch: 233, Train Loss: 0.0545, Val Loss: 0.0517, Val RMSE: 0.2274 Val R2: 0.1435\n",
      "Epoch: 234, Train Loss: 0.0545, Val Loss: 0.0517, Val RMSE: 0.2274 Val R2: 0.1437\n",
      "Epoch: 235, Train Loss: 0.0546, Val Loss: 0.0517, Val RMSE: 0.2274 Val R2: 0.1437\n",
      "Epoch: 236, Train Loss: 0.0546, Val Loss: 0.0517, Val RMSE: 0.2274 Val R2: 0.1437\n",
      "Epoch: 237, Train Loss: 0.0546, Val Loss: 0.0517, Val RMSE: 0.2274 Val R2: 0.1435\n",
      "Epoch: 238, Train Loss: 0.0544, Val Loss: 0.0517, Val RMSE: 0.2274 Val R2: 0.1435\n",
      "Epoch: 239, Train Loss: 0.0544, Val Loss: 0.0517, Val RMSE: 0.2274 Val R2: 0.1434\n",
      "Epoch: 240, Train Loss: 0.0545, Val Loss: 0.0517, Val RMSE: 0.2274 Val R2: 0.1434\n",
      "Epoch: 241, Train Loss: 0.0544, Val Loss: 0.0517, Val RMSE: 0.2274 Val R2: 0.1433\n",
      "Epoch: 242, Train Loss: 0.0544, Val Loss: 0.0517, Val RMSE: 0.2274 Val R2: 0.1433\n",
      "Epoch: 243, Train Loss: 0.0544, Val Loss: 0.0517, Val RMSE: 0.2274 Val R2: 0.1432\n",
      "Epoch: 244, Train Loss: 0.0545, Val Loss: 0.0517, Val RMSE: 0.2275 Val R2: 0.1432\n",
      "Epoch: 245, Train Loss: 0.0544, Val Loss: 0.0517, Val RMSE: 0.2275 Val R2: 0.1432\n",
      "Epoch: 246, Train Loss: 0.0545, Val Loss: 0.0517, Val RMSE: 0.2275 Val R2: 0.1432\n",
      "Epoch: 247, Train Loss: 0.0544, Val Loss: 0.0517, Val RMSE: 0.2274 Val R2: 0.1432\n",
      "Epoch: 248, Train Loss: 0.0544, Val Loss: 0.0517, Val RMSE: 0.2274 Val R2: 0.1434\n",
      "Epoch: 249, Train Loss: 0.0544, Val Loss: 0.0517, Val RMSE: 0.2274 Val R2: 0.1436\n",
      "Epoch: 250, Train Loss: 0.0545, Val Loss: 0.0517, Val RMSE: 0.2274 Val R2: 0.1437\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 41\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[0;32m     40\u001b[0m     train_loss \u001b[39m=\u001b[39m train_model(model, data, data\u001b[39m.\u001b[39mtrain_mask)\n\u001b[1;32m---> 41\u001b[0m     val_mse, val_rmse, val_mae, r2 \u001b[39m=\u001b[39m evaluate_model(model, data, data\u001b[39m.\u001b[39;49mval_mask)\n\u001b[0;32m     43\u001b[0m     train_losses\u001b[39m.\u001b[39mappend(train_loss)\n\u001b[0;32m     44\u001b[0m     val_losses\u001b[39m.\u001b[39mappend(val_mse)\n",
      "Cell \u001b[1;32mIn[10], line 26\u001b[0m, in \u001b[0;36mevaluate_model\u001b[1;34m(model, data, mask)\u001b[0m\n\u001b[0;32m     24\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[0;32m     25\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m---> 26\u001b[0m     out \u001b[39m=\u001b[39m model(data\u001b[39m.\u001b[39;49mx, data\u001b[39m.\u001b[39;49medge_index, data\u001b[39m.\u001b[39;49medge_attributes)[mask]\n\u001b[0;32m     27\u001b[0m     mse \u001b[39m=\u001b[39m criterion(out, data\u001b[39m.\u001b[39my[mask]\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m))\n\u001b[0;32m     28\u001b[0m     r2 \u001b[39m=\u001b[39m r2_score(data\u001b[39m.\u001b[39my[mask]\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m), out)\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\OneDrive - Universidad Católica de Chile\\Documentos\\Tesis_Master\\tesisenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\OneDrive - Universidad Católica de Chile\\Documentos\\Tesis_Master\\Models\\GAT.py:52\u001b[0m, in \u001b[0;36mGATRegression.forward\u001b[1;34m(self, x, edge_index, edge_attr)\u001b[0m\n\u001b[0;32m     50\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1(x, edge_index, edge_attr)\n\u001b[0;32m     51\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(x, edge_index)\n\u001b[0;32m     53\u001b[0m x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39melu(x)\n\u001b[0;32m     54\u001b[0m \u001b[39mfor\u001b[39;00m conv \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconvs:\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\OneDrive - Universidad Católica de Chile\\Documentos\\Tesis_Master\\tesisenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\OneDrive - Universidad Católica de Chile\\Documentos\\Tesis_Master\\tesisenv\\Lib\\site-packages\\torch_geometric\\nn\\conv\\gat_conv.py:225\u001b[0m, in \u001b[0;36mGATConv.forward\u001b[1;34m(self, x, edge_index, edge_attr, size, return_attention_weights)\u001b[0m\n\u001b[0;32m    221\u001b[0m x \u001b[39m=\u001b[39m (x_src, x_dst)\n\u001b[0;32m    223\u001b[0m \u001b[39m# Next, we compute node-level attention coefficients, both for source\u001b[39;00m\n\u001b[0;32m    224\u001b[0m \u001b[39m# and target nodes (if present):\u001b[39;00m\n\u001b[1;32m--> 225\u001b[0m alpha_src \u001b[39m=\u001b[39m (x_src \u001b[39m*\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49matt_src)\u001b[39m.\u001b[39;49msum(dim\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[0;32m    226\u001b[0m alpha_dst \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39mif\u001b[39;00m x_dst \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m (x_dst \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39matt_dst)\u001b[39m.\u001b[39msum(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m    227\u001b[0m alpha \u001b[39m=\u001b[39m (alpha_src, alpha_dst)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "in_features = data.x.shape[1]\n",
    "hidden_features = 16\n",
    "out_features = 1\n",
    "num_layers = 2\n",
    "epochs = 300\n",
    "lr = 0.01\n",
    "num_heads = 6\n",
    "\n",
    "model = GATRegression(in_features, hidden_features, out_features, edge_dim=None, heads=num_heads, num_layers=2)\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "def train_model(model, data, mask):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index, data.edge_attributes)[mask]\n",
    "    loss = criterion(out, data.y[mask].view(-1, 1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "def evaluate_model(model, data, mask):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data.x, data.edge_index, data.edge_attributes)[mask]\n",
    "        mse = criterion(out, data.y[mask].view(-1, 1))\n",
    "        r2 = r2_score(data.y[mask].view(-1, 1), out)\n",
    "        mae = mean_absolute_error(out, data.y[mask].view(-1, 1))\n",
    "        rmse = torch.sqrt(mse)\n",
    "    return mse.item(), rmse, mae, r2\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_maes = []\n",
    "val_maes = []\n",
    "val_rmses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train_model(model, data, data.train_mask)\n",
    "    val_mse, val_rmse, val_mae, r2 = evaluate_model(model, data, data.val_mask)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_mse)\n",
    "    val_maes.append(val_mae)\n",
    "    val_rmses.append(val_rmse)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:03d}, Train Loss: {train_loss:.4f}, Val Loss: {val_mse:.4f}, Val RMSE: {val_rmse:.4f} Val R2: {r2:.4f}')\n",
    "\n",
    "test_mse, test_rmse, test_mae, test_r2 = evaluate_model(model, data, data.test_mask)\n",
    "print(f'Test RMSE: {test_rmse:.4f}, Test MAE: {test_mae:.4f}, Test R2: {test_r2:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TransfomerConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Train Loss: 1710.4404, Val Loss: 32883.6133, Val RMSE: 181.3384 Val R2: -521872.3368\n",
      "Epoch: 002, Train Loss: 62878.1914, Val Loss: 694.4125, Val RMSE: 26.3517 Val R2: -11019.5474\n",
      "Epoch: 003, Train Loss: 1392.1504, Val Loss: 1406.0488, Val RMSE: 37.4973 Val R2: -22313.4406\n",
      "Epoch: 004, Train Loss: 2522.0806, Val Loss: 3608.8936, Val RMSE: 60.0741 Val R2: -57273.2940\n",
      "Epoch: 005, Train Loss: 6727.0889, Val Loss: 2542.9031, Val RMSE: 50.4272 Val R2: -40355.6841\n",
      "Epoch: 006, Train Loss: 4895.0356, Val Loss: 543.6850, Val RMSE: 23.3171 Val R2: -8627.4532\n",
      "Epoch: 007, Train Loss: 1476.9976, Val Loss: 398.5207, Val RMSE: 19.9630 Val R2: -6323.6500\n",
      "Epoch: 008, Train Loss: 1805.5237, Val Loss: 1256.2605, Val RMSE: 35.4438 Val R2: -19936.2533\n",
      "Epoch: 009, Train Loss: 3804.2686, Val Loss: 729.2445, Val RMSE: 27.0045 Val R2: -11572.3410\n",
      "Epoch: 010, Train Loss: 2577.4172, Val Loss: 95.8467, Val RMSE: 9.7901 Val R2: -1520.1174\n",
      "Epoch: 011, Train Loss: 1085.4829, Val Loss: 86.2536, Val RMSE: 9.2873 Val R2: -1367.8723\n",
      "Epoch: 012, Train Loss: 709.9132, Val Loss: 463.5639, Val RMSE: 21.5305 Val R2: -7355.9058\n",
      "Epoch: 013, Train Loss: 1169.6055, Val Loss: 707.0211, Val RMSE: 26.5899 Val R2: -11219.6496\n",
      "Epoch: 014, Train Loss: 1551.0140, Val Loss: 605.9021, Val RMSE: 24.6151 Val R2: -9614.8590\n",
      "Epoch: 015, Train Loss: 1415.6036, Val Loss: 304.8547, Val RMSE: 17.4601 Val R2: -4837.1415\n",
      "Epoch: 016, Train Loss: 929.5638, Val Loss: 63.8246, Val RMSE: 7.9890 Val R2: -1011.9160\n",
      "Epoch: 017, Train Loss: 544.5396, Val Loss: 50.0204, Val RMSE: 7.0725 Val R2: -792.8398\n",
      "Epoch: 018, Train Loss: 589.5671, Val Loss: 129.1192, Val RMSE: 11.3631 Val R2: -2048.1633\n",
      "Epoch: 019, Train Loss: 675.4084, Val Loss: 152.6167, Val RMSE: 12.3538 Val R2: -2421.0749\n",
      "Epoch: 020, Train Loss: 616.7764, Val Loss: 128.4541, Val RMSE: 11.3338 Val R2: -2037.6084\n",
      "Epoch: 021, Train Loss: 511.5768, Val Loss: 101.4282, Val RMSE: 10.0712 Val R2: -1608.6972\n",
      "Epoch: 022, Train Loss: 419.4831, Val Loss: 87.9970, Val RMSE: 9.3807 Val R2: -1395.5408\n",
      "Epoch: 023, Train Loss: 388.6250, Val Loss: 81.2545, Val RMSE: 9.0141 Val R2: -1288.5341\n",
      "Epoch: 024, Train Loss: 356.3227, Val Loss: 76.6523, Val RMSE: 8.7551 Val R2: -1215.4959\n",
      "Epoch: 025, Train Loss: 338.1256, Val Loss: 71.3248, Val RMSE: 8.4454 Val R2: -1130.9474\n",
      "Epoch: 026, Train Loss: 307.1407, Val Loss: 63.9404, Val RMSE: 7.9963 Val R2: -1013.7551\n",
      "Epoch: 027, Train Loss: 288.3892, Val Loss: 55.8615, Val RMSE: 7.4741 Val R2: -885.5390\n",
      "Epoch: 028, Train Loss: 234.9297, Val Loss: 54.5802, Val RMSE: 7.3878 Val R2: -865.2052\n",
      "Epoch: 029, Train Loss: 212.5468, Val Loss: 63.3478, Val RMSE: 7.9591 Val R2: -1004.3500\n",
      "Epoch: 030, Train Loss: 234.2111, Val Loss: 61.1387, Val RMSE: 7.8191 Val R2: -969.2902\n",
      "Epoch: 031, Train Loss: 204.5471, Val Loss: 53.2935, Val RMSE: 7.3002 Val R2: -844.7845\n",
      "Epoch: 032, Train Loss: 175.9375, Val Loss: 48.0672, Val RMSE: 6.9331 Val R2: -761.8421\n",
      "Epoch: 033, Train Loss: 164.7054, Val Loss: 45.0890, Val RMSE: 6.7148 Val R2: -714.5760\n",
      "Epoch: 034, Train Loss: 152.6161, Val Loss: 43.2024, Val RMSE: 6.5729 Val R2: -684.6353\n",
      "Epoch: 035, Train Loss: 141.6470, Val Loss: 42.1853, Val RMSE: 6.4950 Val R2: -668.4946\n",
      "Epoch: 036, Train Loss: 133.9575, Val Loss: 41.2100, Val RMSE: 6.4195 Val R2: -653.0160\n",
      "Epoch: 037, Train Loss: 123.3946, Val Loss: 39.8646, Val RMSE: 6.3138 Val R2: -631.6637\n",
      "Epoch: 038, Train Loss: 116.4395, Val Loss: 37.3934, Val RMSE: 6.1150 Val R2: -592.4446\n",
      "Epoch: 039, Train Loss: 108.4054, Val Loss: 33.7842, Val RMSE: 5.8124 Val R2: -535.1657\n",
      "Epoch: 040, Train Loss: 102.9217, Val Loss: 29.0266, Val RMSE: 5.3876 Val R2: -459.6615\n",
      "Epoch: 041, Train Loss: 93.6622, Val Loss: 23.4482, Val RMSE: 4.8423 Val R2: -371.1307\n",
      "Epoch: 042, Train Loss: 87.5696, Val Loss: 17.5667, Val RMSE: 4.1913 Val R2: -277.7886\n",
      "Epoch: 043, Train Loss: 82.7002, Val Loss: 12.3752, Val RMSE: 3.5178 Val R2: -195.3984\n",
      "Epoch: 044, Train Loss: 78.5786, Val Loss: 8.8978, Val RMSE: 2.9829 Val R2: -140.2105\n",
      "Epoch: 045, Train Loss: 78.4711, Val Loss: 7.0768, Val RMSE: 2.6602 Val R2: -111.3116\n",
      "Epoch: 046, Train Loss: 75.8254, Val Loss: 6.2588, Val RMSE: 2.5018 Val R2: -98.3286\n",
      "Epoch: 047, Train Loss: 75.2202, Val Loss: 6.0445, Val RMSE: 2.4586 Val R2: -94.9280\n",
      "Epoch: 048, Train Loss: 71.4290, Val Loss: 6.2649, Val RMSE: 2.5030 Val R2: -98.4260\n",
      "Epoch: 049, Train Loss: 67.3437, Val Loss: 6.7240, Val RMSE: 2.5931 Val R2: -105.7123\n",
      "Epoch: 050, Train Loss: 66.9080, Val Loss: 7.1760, Val RMSE: 2.6788 Val R2: -112.8855\n",
      "Epoch: 051, Train Loss: 65.1252, Val Loss: 7.5403, Val RMSE: 2.7460 Val R2: -118.6666\n",
      "Epoch: 052, Train Loss: 63.9191, Val Loss: 7.7699, Val RMSE: 2.7875 Val R2: -122.3113\n",
      "Epoch: 053, Train Loss: 62.4035, Val Loss: 7.8636, Val RMSE: 2.8042 Val R2: -123.7983\n",
      "Epoch: 054, Train Loss: 60.4341, Val Loss: 7.7982, Val RMSE: 2.7925 Val R2: -122.7597\n",
      "Epoch: 055, Train Loss: 59.5609, Val Loss: 7.6267, Val RMSE: 2.7616 Val R2: -120.0375\n",
      "Epoch: 056, Train Loss: 58.5024, Val Loss: 7.3924, Val RMSE: 2.7189 Val R2: -116.3195\n",
      "Epoch: 057, Train Loss: 56.3896, Val Loss: 7.1010, Val RMSE: 2.6648 Val R2: -111.6954\n",
      "Epoch: 058, Train Loss: 54.1278, Val Loss: 6.7939, Val RMSE: 2.6065 Val R2: -106.8211\n",
      "Epoch: 059, Train Loss: 52.6859, Val Loss: 6.4730, Val RMSE: 2.5442 Val R2: -101.7278\n",
      "Epoch: 060, Train Loss: 52.7738, Val Loss: 6.1445, Val RMSE: 2.4788 Val R2: -96.5144\n",
      "Epoch: 061, Train Loss: 49.7989, Val Loss: 5.8497, Val RMSE: 2.4186 Val R2: -91.8360\n",
      "Epoch: 062, Train Loss: 48.2146, Val Loss: 5.5942, Val RMSE: 2.3652 Val R2: -87.7813\n",
      "Epoch: 063, Train Loss: 46.9094, Val Loss: 5.3806, Val RMSE: 2.3196 Val R2: -84.3921\n",
      "Epoch: 064, Train Loss: 46.5240, Val Loss: 5.2216, Val RMSE: 2.2851 Val R2: -81.8692\n",
      "Epoch: 065, Train Loss: 44.4961, Val Loss: 5.0955, Val RMSE: 2.2573 Val R2: -79.8671\n",
      "Epoch: 066, Train Loss: 43.9126, Val Loss: 4.9988, Val RMSE: 2.2358 Val R2: -78.3330\n",
      "Epoch: 067, Train Loss: 42.9498, Val Loss: 4.9146, Val RMSE: 2.2169 Val R2: -76.9956\n",
      "Epoch: 068, Train Loss: 42.0615, Val Loss: 4.8306, Val RMSE: 2.1979 Val R2: -75.6639\n",
      "Epoch: 069, Train Loss: 40.8546, Val Loss: 4.7303, Val RMSE: 2.1749 Val R2: -74.0709\n",
      "Epoch: 070, Train Loss: 40.7604, Val Loss: 4.6118, Val RMSE: 2.1475 Val R2: -72.1912\n",
      "Epoch: 071, Train Loss: 39.4760, Val Loss: 4.4578, Val RMSE: 2.1114 Val R2: -69.7471\n",
      "Epoch: 072, Train Loss: 39.0880, Val Loss: 4.2837, Val RMSE: 2.0697 Val R2: -66.9840\n",
      "Epoch: 073, Train Loss: 38.3245, Val Loss: 4.1082, Val RMSE: 2.0269 Val R2: -64.1992\n",
      "Epoch: 074, Train Loss: 38.2996, Val Loss: 3.9517, Val RMSE: 1.9879 Val R2: -61.7146\n",
      "Epoch: 075, Train Loss: 37.0633, Val Loss: 3.8157, Val RMSE: 1.9534 Val R2: -59.5560\n",
      "Epoch: 076, Train Loss: 37.4546, Val Loss: 3.6904, Val RMSE: 1.9210 Val R2: -57.5680\n",
      "Epoch: 077, Train Loss: 36.5579, Val Loss: 3.5904, Val RMSE: 1.8948 Val R2: -55.9804\n",
      "Epoch: 078, Train Loss: 35.5061, Val Loss: 3.5007, Val RMSE: 1.8710 Val R2: -54.5572\n",
      "Epoch: 079, Train Loss: 35.0145, Val Loss: 3.4273, Val RMSE: 1.8513 Val R2: -53.3927\n",
      "Epoch: 080, Train Loss: 34.7852, Val Loss: 3.3668, Val RMSE: 1.8349 Val R2: -52.4328\n",
      "Epoch: 081, Train Loss: 34.5042, Val Loss: 3.2970, Val RMSE: 1.8158 Val R2: -51.3240\n",
      "Epoch: 082, Train Loss: 33.4929, Val Loss: 3.2318, Val RMSE: 1.7977 Val R2: -50.2905\n",
      "Epoch: 083, Train Loss: 33.4935, Val Loss: 3.1683, Val RMSE: 1.7800 Val R2: -49.2814\n",
      "Epoch: 084, Train Loss: 32.6585, Val Loss: 3.0894, Val RMSE: 1.7577 Val R2: -48.0302\n",
      "Epoch: 085, Train Loss: 32.5947, Val Loss: 3.0041, Val RMSE: 1.7332 Val R2: -46.6760\n",
      "Epoch: 086, Train Loss: 32.0564, Val Loss: 2.9135, Val RMSE: 1.7069 Val R2: -45.2383\n",
      "Epoch: 087, Train Loss: 31.2486, Val Loss: 2.8139, Val RMSE: 1.6775 Val R2: -43.6577\n",
      "Epoch: 088, Train Loss: 31.3222, Val Loss: 2.7156, Val RMSE: 1.6479 Val R2: -42.0976\n",
      "Epoch: 089, Train Loss: 30.4181, Val Loss: 2.6216, Val RMSE: 1.6191 Val R2: -40.6060\n",
      "Epoch: 090, Train Loss: 30.1760, Val Loss: 2.5358, Val RMSE: 1.5924 Val R2: -39.2438\n",
      "Epoch: 091, Train Loss: 29.8291, Val Loss: 2.4534, Val RMSE: 1.5663 Val R2: -37.9365\n",
      "Epoch: 092, Train Loss: 29.8180, Val Loss: 2.3802, Val RMSE: 1.5428 Val R2: -36.7749\n",
      "Epoch: 093, Train Loss: 29.2454, Val Loss: 2.3254, Val RMSE: 1.5249 Val R2: -35.9056\n",
      "Epoch: 094, Train Loss: 29.0582, Val Loss: 2.2777, Val RMSE: 1.5092 Val R2: -35.1479\n",
      "Epoch: 095, Train Loss: 28.8290, Val Loss: 2.2525, Val RMSE: 1.5008 Val R2: -34.7483\n",
      "Epoch: 096, Train Loss: 28.5048, Val Loss: 2.2388, Val RMSE: 1.4963 Val R2: -34.5308\n",
      "Epoch: 097, Train Loss: 28.1594, Val Loss: 2.2332, Val RMSE: 1.4944 Val R2: -34.4423\n",
      "Epoch: 098, Train Loss: 28.1847, Val Loss: 2.2325, Val RMSE: 1.4942 Val R2: -34.4310\n",
      "Epoch: 099, Train Loss: 27.4241, Val Loss: 2.2333, Val RMSE: 1.4944 Val R2: -34.4433\n",
      "Epoch: 100, Train Loss: 27.5154, Val Loss: 2.2253, Val RMSE: 1.4918 Val R2: -34.3170\n",
      "Epoch: 101, Train Loss: 27.2876, Val Loss: 2.2069, Val RMSE: 1.4856 Val R2: -34.0237\n",
      "Epoch: 102, Train Loss: 27.2087, Val Loss: 2.1803, Val RMSE: 1.4766 Val R2: -33.6021\n",
      "Epoch: 103, Train Loss: 26.6559, Val Loss: 2.1401, Val RMSE: 1.4629 Val R2: -32.9642\n",
      "Epoch: 104, Train Loss: 26.5818, Val Loss: 2.0906, Val RMSE: 1.4459 Val R2: -32.1778\n",
      "Epoch: 105, Train Loss: 26.2746, Val Loss: 2.0383, Val RMSE: 1.4277 Val R2: -31.3480\n",
      "Epoch: 106, Train Loss: 25.9921, Val Loss: 1.9829, Val RMSE: 1.4082 Val R2: -30.4699\n",
      "Epoch: 107, Train Loss: 25.7430, Val Loss: 1.9259, Val RMSE: 1.3878 Val R2: -29.5642\n",
      "Epoch: 108, Train Loss: 25.5614, Val Loss: 1.8694, Val RMSE: 1.3673 Val R2: -28.6677\n",
      "Epoch: 109, Train Loss: 25.5042, Val Loss: 1.8231, Val RMSE: 1.3502 Val R2: -27.9336\n",
      "Epoch: 110, Train Loss: 24.9816, Val Loss: 1.7827, Val RMSE: 1.3352 Val R2: -27.2926\n",
      "Epoch: 111, Train Loss: 24.7249, Val Loss: 1.7462, Val RMSE: 1.3214 Val R2: -26.7125\n",
      "Epoch: 112, Train Loss: 24.7200, Val Loss: 1.7146, Val RMSE: 1.3094 Val R2: -26.2105\n",
      "Epoch: 113, Train Loss: 24.6498, Val Loss: 1.6928, Val RMSE: 1.3011 Val R2: -25.8648\n",
      "Epoch: 114, Train Loss: 24.0804, Val Loss: 1.6709, Val RMSE: 1.2926 Val R2: -25.5182\n",
      "Epoch: 115, Train Loss: 24.0604, Val Loss: 1.6516, Val RMSE: 1.2852 Val R2: -25.2120\n",
      "Epoch: 116, Train Loss: 24.0524, Val Loss: 1.6396, Val RMSE: 1.2805 Val R2: -25.0210\n",
      "Epoch: 117, Train Loss: 23.5021, Val Loss: 1.6264, Val RMSE: 1.2753 Val R2: -24.8110\n",
      "Epoch: 118, Train Loss: 23.2728, Val Loss: 1.6120, Val RMSE: 1.2697 Val R2: -24.5832\n",
      "Epoch: 119, Train Loss: 23.5484, Val Loss: 1.5963, Val RMSE: 1.2634 Val R2: -24.3333\n",
      "Epoch: 120, Train Loss: 23.3336, Val Loss: 1.5807, Val RMSE: 1.2573 Val R2: -24.0863\n",
      "Epoch: 121, Train Loss: 23.1013, Val Loss: 1.5595, Val RMSE: 1.2488 Val R2: -23.7501\n",
      "Epoch: 122, Train Loss: 23.0057, Val Loss: 1.5343, Val RMSE: 1.2387 Val R2: -23.3491\n",
      "Epoch: 123, Train Loss: 22.7975, Val Loss: 1.5068, Val RMSE: 1.2275 Val R2: -22.9131\n",
      "Epoch: 124, Train Loss: 22.6359, Val Loss: 1.4779, Val RMSE: 1.2157 Val R2: -22.4553\n",
      "Epoch: 125, Train Loss: 22.5018, Val Loss: 1.4510, Val RMSE: 1.2046 Val R2: -22.0283\n",
      "Epoch: 126, Train Loss: 21.9384, Val Loss: 1.4245, Val RMSE: 1.1935 Val R2: -21.6075\n",
      "Epoch: 127, Train Loss: 22.3121, Val Loss: 1.4038, Val RMSE: 1.1848 Val R2: -21.2789\n",
      "Epoch: 128, Train Loss: 22.1241, Val Loss: 1.3878, Val RMSE: 1.1780 Val R2: -21.0248\n",
      "Epoch: 129, Train Loss: 21.7148, Val Loss: 1.3815, Val RMSE: 1.1754 Val R2: -20.9254\n",
      "Epoch: 130, Train Loss: 21.5014, Val Loss: 1.3805, Val RMSE: 1.1750 Val R2: -20.9095\n",
      "Epoch: 131, Train Loss: 21.2937, Val Loss: 1.3811, Val RMSE: 1.1752 Val R2: -20.9187\n",
      "Epoch: 132, Train Loss: 21.3599, Val Loss: 1.3799, Val RMSE: 1.1747 Val R2: -20.8995\n",
      "Epoch: 133, Train Loss: 21.3981, Val Loss: 1.3714, Val RMSE: 1.1711 Val R2: -20.7649\n",
      "Epoch: 134, Train Loss: 21.1465, Val Loss: 1.3575, Val RMSE: 1.1651 Val R2: -20.5438\n",
      "Epoch: 135, Train Loss: 20.8833, Val Loss: 1.3400, Val RMSE: 1.1576 Val R2: -20.2658\n",
      "Epoch: 136, Train Loss: 20.6172, Val Loss: 1.3233, Val RMSE: 1.1503 Val R2: -20.0004\n",
      "Epoch: 137, Train Loss: 20.5974, Val Loss: 1.3051, Val RMSE: 1.1424 Val R2: -19.7124\n",
      "Epoch: 138, Train Loss: 20.6003, Val Loss: 1.2911, Val RMSE: 1.1363 Val R2: -19.4908\n",
      "Epoch: 139, Train Loss: 20.4398, Val Loss: 1.2804, Val RMSE: 1.1316 Val R2: -19.3205\n",
      "Epoch: 140, Train Loss: 20.1317, Val Loss: 1.2724, Val RMSE: 1.1280 Val R2: -19.1928\n",
      "Epoch: 141, Train Loss: 20.2100, Val Loss: 1.2687, Val RMSE: 1.1264 Val R2: -19.1342\n",
      "Epoch: 142, Train Loss: 20.0237, Val Loss: 1.2667, Val RMSE: 1.1255 Val R2: -19.1034\n",
      "Epoch: 143, Train Loss: 19.9915, Val Loss: 1.2599, Val RMSE: 1.1224 Val R2: -18.9949\n",
      "Epoch: 144, Train Loss: 19.9211, Val Loss: 1.2530, Val RMSE: 1.1194 Val R2: -18.8851\n",
      "Epoch: 145, Train Loss: 19.7729, Val Loss: 1.2400, Val RMSE: 1.1136 Val R2: -18.6798\n",
      "Epoch: 146, Train Loss: 19.4226, Val Loss: 1.2262, Val RMSE: 1.1073 Val R2: -18.4598\n",
      "Epoch: 147, Train Loss: 19.3237, Val Loss: 1.2071, Val RMSE: 1.0987 Val R2: -18.1576\n",
      "Epoch: 148, Train Loss: 19.4146, Val Loss: 1.1869, Val RMSE: 1.0894 Val R2: -17.8361\n",
      "Epoch: 149, Train Loss: 19.2068, Val Loss: 1.1664, Val RMSE: 1.0800 Val R2: -17.5113\n",
      "Epoch: 150, Train Loss: 19.1465, Val Loss: 1.1372, Val RMSE: 1.0664 Val R2: -17.0478\n",
      "Test RMSE: 1.0644, Test MAE: 0.8993, Test R2: -16.9327\n"
     ]
    }
   ],
   "source": [
    "in_features = data.x.shape[1]\n",
    "hidden_features = 64\n",
    "out_features = 1\n",
    "num_layers = 2\n",
    "epochs = 150\n",
    "lr = 0.1\n",
    "num_heads = 2\n",
    "\n",
    "model = TransformerRegression(in_features, hidden_features, out_features, edge_dim=2, heads=num_heads)\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "def train_model(model, data, mask):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index, data.edge_attributes)[mask]\n",
    "    loss = criterion(out, data.y[mask].view(-1, 1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "def evaluate_model(model, data, mask):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data.x, data.edge_index, data.edge_attributes)[mask]\n",
    "        mse = criterion(out, data.y[mask].view(-1, 1))\n",
    "        r2 = r2_score(data.y[mask].view(-1, 1), out)\n",
    "        mae = mean_absolute_error(out, data.y[mask].view(-1, 1))\n",
    "        rmse = torch.sqrt(mse)\n",
    "    return mse.item(), rmse, mae, r2\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_maes = []\n",
    "val_maes = []\n",
    "val_rmses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train_model(model, data, data.train_mask)\n",
    "    val_mse, val_rmse, val_mae, r2 = evaluate_model(model, data, data.val_mask)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_mse)\n",
    "    val_maes.append(val_mae)\n",
    "    val_rmses.append(val_rmse)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:03d}, Train Loss: {train_loss:.4f}, Val Loss: {val_mse:.4f}, Val RMSE: {val_rmse:.4f} Val R2: {r2:.4f}')\n",
    "\n",
    "test_mse, test_rmse, test_mae, test_r2 = evaluate_model(model, data, data.test_mask)\n",
    "print(f'Test RMSE: {test_rmse:.4f}, Test MAE: {test_mae:.4f}, Test R2: {test_r2:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Loss')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GATv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Train Loss: 0.3370, Val Loss: 2.1337, Val RMSE: 1.4607 Val R2: -34.3370\n",
      "Epoch: 002, Train Loss: 2.7946, Val Loss: 0.4203, Val RMSE: 0.6483 Val R2: -5.9610\n",
      "Epoch: 003, Train Loss: 0.5397, Val Loss: 0.9005, Val RMSE: 0.9489 Val R2: -13.9131\n",
      "Epoch: 004, Train Loss: 0.9921, Val Loss: 0.7203, Val RMSE: 0.8487 Val R2: -10.9287\n",
      "Epoch: 005, Train Loss: 0.7518, Val Loss: 0.4639, Val RMSE: 0.6811 Val R2: -6.6838\n",
      "Epoch: 006, Train Loss: 0.5063, Val Loss: 0.3861, Val RMSE: 0.6214 Val R2: -5.3951\n",
      "Epoch: 007, Train Loss: 0.4831, Val Loss: 0.2938, Val RMSE: 0.5421 Val R2: -3.8664\n",
      "Epoch: 008, Train Loss: 0.4089, Val Loss: 0.1701, Val RMSE: 0.4124 Val R2: -1.8172\n",
      "Epoch: 009, Train Loss: 0.2822, Val Loss: 0.1103, Val RMSE: 0.3321 Val R2: -0.8271\n",
      "Epoch: 010, Train Loss: 0.2112, Val Loss: 0.1422, Val RMSE: 0.3770 Val R2: -1.3543\n",
      "Epoch: 011, Train Loss: 0.2256, Val Loss: 0.2022, Val RMSE: 0.4497 Val R2: -2.3490\n",
      "Epoch: 012, Train Loss: 0.2684, Val Loss: 0.2243, Val RMSE: 0.4736 Val R2: -2.7143\n",
      "Epoch: 013, Train Loss: 0.2768, Val Loss: 0.1947, Val RMSE: 0.4412 Val R2: -2.2246\n",
      "Epoch: 014, Train Loss: 0.2410, Val Loss: 0.1402, Val RMSE: 0.3744 Val R2: -1.3218\n",
      "Epoch: 015, Train Loss: 0.1781, Val Loss: 0.0911, Val RMSE: 0.3018 Val R2: -0.5081\n",
      "Epoch: 016, Train Loss: 0.1344, Val Loss: 0.0623, Val RMSE: 0.2496 Val R2: -0.0320\n",
      "Epoch: 017, Train Loss: 0.1111, Val Loss: 0.0549, Val RMSE: 0.2344 Val R2: 0.0900\n",
      "Epoch: 018, Train Loss: 0.1059, Val Loss: 0.0621, Val RMSE: 0.2492 Val R2: -0.0284\n",
      "Epoch: 019, Train Loss: 0.1136, Val Loss: 0.0750, Val RMSE: 0.2739 Val R2: -0.2423\n",
      "Epoch: 020, Train Loss: 0.1249, Val Loss: 0.0869, Val RMSE: 0.2947 Val R2: -0.4384\n",
      "Epoch: 021, Train Loss: 0.1344, Val Loss: 0.0939, Val RMSE: 0.3065 Val R2: -0.5556\n",
      "Epoch: 022, Train Loss: 0.1378, Val Loss: 0.0957, Val RMSE: 0.3094 Val R2: -0.5858\n",
      "Epoch: 023, Train Loss: 0.1367, Val Loss: 0.0935, Val RMSE: 0.3059 Val R2: -0.5493\n",
      "Epoch: 024, Train Loss: 0.1315, Val Loss: 0.0892, Val RMSE: 0.2987 Val R2: -0.4779\n",
      "Epoch: 025, Train Loss: 0.1249, Val Loss: 0.0844, Val RMSE: 0.2906 Val R2: -0.3984\n",
      "Epoch: 026, Train Loss: 0.1173, Val Loss: 0.0801, Val RMSE: 0.2831 Val R2: -0.3270\n",
      "Epoch: 027, Train Loss: 0.1113, Val Loss: 0.0765, Val RMSE: 0.2766 Val R2: -0.2667\n",
      "Epoch: 028, Train Loss: 0.1051, Val Loss: 0.0733, Val RMSE: 0.2708 Val R2: -0.2147\n",
      "Epoch: 029, Train Loss: 0.1006, Val Loss: 0.0704, Val RMSE: 0.2653 Val R2: -0.1659\n",
      "Epoch: 030, Train Loss: 0.0965, Val Loss: 0.0675, Val RMSE: 0.2597 Val R2: -0.1171\n",
      "Epoch: 031, Train Loss: 0.0933, Val Loss: 0.0645, Val RMSE: 0.2540 Val R2: -0.0682\n",
      "Epoch: 032, Train Loss: 0.0902, Val Loss: 0.0617, Val RMSE: 0.2483 Val R2: -0.0211\n",
      "Epoch: 033, Train Loss: 0.0878, Val Loss: 0.0591, Val RMSE: 0.2431 Val R2: 0.0211\n",
      "Epoch: 034, Train Loss: 0.0862, Val Loss: 0.0570, Val RMSE: 0.2388 Val R2: 0.0554\n",
      "Epoch: 035, Train Loss: 0.0848, Val Loss: 0.0555, Val RMSE: 0.2356 Val R2: 0.0807\n",
      "Epoch: 036, Train Loss: 0.0843, Val Loss: 0.0545, Val RMSE: 0.2334 Val R2: 0.0977\n",
      "Epoch: 037, Train Loss: 0.0840, Val Loss: 0.0538, Val RMSE: 0.2320 Val R2: 0.1083\n",
      "Epoch: 038, Train Loss: 0.0835, Val Loss: 0.0535, Val RMSE: 0.2312 Val R2: 0.1145\n",
      "Epoch: 039, Train Loss: 0.0837, Val Loss: 0.0532, Val RMSE: 0.2307 Val R2: 0.1184\n",
      "Epoch: 040, Train Loss: 0.0830, Val Loss: 0.0531, Val RMSE: 0.2305 Val R2: 0.1204\n",
      "Epoch: 041, Train Loss: 0.0821, Val Loss: 0.0531, Val RMSE: 0.2304 Val R2: 0.1206\n",
      "Epoch: 042, Train Loss: 0.0816, Val Loss: 0.0532, Val RMSE: 0.2307 Val R2: 0.1182\n",
      "Epoch: 043, Train Loss: 0.0800, Val Loss: 0.0536, Val RMSE: 0.2314 Val R2: 0.1128\n",
      "Epoch: 044, Train Loss: 0.0792, Val Loss: 0.0541, Val RMSE: 0.2325 Val R2: 0.1044\n",
      "Epoch: 045, Train Loss: 0.0783, Val Loss: 0.0547, Val RMSE: 0.2340 Val R2: 0.0935\n",
      "Epoch: 046, Train Loss: 0.0773, Val Loss: 0.0554, Val RMSE: 0.2355 Val R2: 0.0818\n",
      "Epoch: 047, Train Loss: 0.0767, Val Loss: 0.0561, Val RMSE: 0.2368 Val R2: 0.0711\n",
      "Epoch: 048, Train Loss: 0.0759, Val Loss: 0.0566, Val RMSE: 0.2378 Val R2: 0.0632\n",
      "Epoch: 049, Train Loss: 0.0752, Val Loss: 0.0568, Val RMSE: 0.2383 Val R2: 0.0592\n",
      "Epoch: 050, Train Loss: 0.0746, Val Loss: 0.0568, Val RMSE: 0.2383 Val R2: 0.0595\n",
      "Epoch: 051, Train Loss: 0.0744, Val Loss: 0.0566, Val RMSE: 0.2378 Val R2: 0.0631\n",
      "Epoch: 052, Train Loss: 0.0737, Val Loss: 0.0562, Val RMSE: 0.2371 Val R2: 0.0693\n",
      "Epoch: 053, Train Loss: 0.0728, Val Loss: 0.0557, Val RMSE: 0.2361 Val R2: 0.0768\n",
      "Epoch: 054, Train Loss: 0.0719, Val Loss: 0.0553, Val RMSE: 0.2351 Val R2: 0.0844\n",
      "Epoch: 055, Train Loss: 0.0714, Val Loss: 0.0549, Val RMSE: 0.2342 Val R2: 0.0914\n",
      "Epoch: 056, Train Loss: 0.0711, Val Loss: 0.0545, Val RMSE: 0.2335 Val R2: 0.0972\n",
      "Epoch: 057, Train Loss: 0.0708, Val Loss: 0.0543, Val RMSE: 0.2329 Val R2: 0.1014\n",
      "Epoch: 058, Train Loss: 0.0703, Val Loss: 0.0541, Val RMSE: 0.2326 Val R2: 0.1040\n",
      "Epoch: 059, Train Loss: 0.0697, Val Loss: 0.0540, Val RMSE: 0.2325 Val R2: 0.1050\n",
      "Epoch: 060, Train Loss: 0.0692, Val Loss: 0.0541, Val RMSE: 0.2325 Val R2: 0.1047\n",
      "Epoch: 061, Train Loss: 0.0690, Val Loss: 0.0542, Val RMSE: 0.2327 Val R2: 0.1031\n",
      "Epoch: 062, Train Loss: 0.0686, Val Loss: 0.0543, Val RMSE: 0.2330 Val R2: 0.1005\n",
      "Epoch: 063, Train Loss: 0.0680, Val Loss: 0.0545, Val RMSE: 0.2335 Val R2: 0.0973\n",
      "Epoch: 064, Train Loss: 0.0680, Val Loss: 0.0547, Val RMSE: 0.2339 Val R2: 0.0938\n",
      "Epoch: 065, Train Loss: 0.0674, Val Loss: 0.0549, Val RMSE: 0.2344 Val R2: 0.0904\n",
      "Epoch: 066, Train Loss: 0.0671, Val Loss: 0.0551, Val RMSE: 0.2347 Val R2: 0.0874\n",
      "Epoch: 067, Train Loss: 0.0669, Val Loss: 0.0552, Val RMSE: 0.2350 Val R2: 0.0852\n",
      "Epoch: 068, Train Loss: 0.0665, Val Loss: 0.0553, Val RMSE: 0.2352 Val R2: 0.0840\n",
      "Epoch: 069, Train Loss: 0.0663, Val Loss: 0.0553, Val RMSE: 0.2352 Val R2: 0.0841\n",
      "Epoch: 070, Train Loss: 0.0662, Val Loss: 0.0552, Val RMSE: 0.2350 Val R2: 0.0854\n",
      "Epoch: 071, Train Loss: 0.0657, Val Loss: 0.0551, Val RMSE: 0.2347 Val R2: 0.0879\n",
      "Epoch: 072, Train Loss: 0.0654, Val Loss: 0.0549, Val RMSE: 0.2342 Val R2: 0.0913\n",
      "Epoch: 073, Train Loss: 0.0650, Val Loss: 0.0546, Val RMSE: 0.2337 Val R2: 0.0952\n",
      "Epoch: 074, Train Loss: 0.0648, Val Loss: 0.0544, Val RMSE: 0.2332 Val R2: 0.0993\n",
      "Epoch: 075, Train Loss: 0.0646, Val Loss: 0.0541, Val RMSE: 0.2327 Val R2: 0.1032\n",
      "Epoch: 076, Train Loss: 0.0641, Val Loss: 0.0539, Val RMSE: 0.2322 Val R2: 0.1067\n",
      "Epoch: 077, Train Loss: 0.0641, Val Loss: 0.0538, Val RMSE: 0.2319 Val R2: 0.1096\n",
      "Epoch: 078, Train Loss: 0.0635, Val Loss: 0.0536, Val RMSE: 0.2316 Val R2: 0.1118\n",
      "Epoch: 079, Train Loss: 0.0634, Val Loss: 0.0535, Val RMSE: 0.2314 Val R2: 0.1134\n",
      "Epoch: 080, Train Loss: 0.0627, Val Loss: 0.0535, Val RMSE: 0.2312 Val R2: 0.1146\n",
      "Epoch: 081, Train Loss: 0.0629, Val Loss: 0.0534, Val RMSE: 0.2311 Val R2: 0.1153\n",
      "Epoch: 082, Train Loss: 0.0627, Val Loss: 0.0534, Val RMSE: 0.2311 Val R2: 0.1155\n",
      "Epoch: 083, Train Loss: 0.0624, Val Loss: 0.0534, Val RMSE: 0.2311 Val R2: 0.1154\n",
      "Epoch: 084, Train Loss: 0.0622, Val Loss: 0.0534, Val RMSE: 0.2312 Val R2: 0.1151\n",
      "Epoch: 085, Train Loss: 0.0619, Val Loss: 0.0535, Val RMSE: 0.2312 Val R2: 0.1147\n",
      "Epoch: 086, Train Loss: 0.0619, Val Loss: 0.0535, Val RMSE: 0.2312 Val R2: 0.1145\n",
      "Epoch: 087, Train Loss: 0.0615, Val Loss: 0.0535, Val RMSE: 0.2312 Val R2: 0.1144\n",
      "Epoch: 088, Train Loss: 0.0615, Val Loss: 0.0535, Val RMSE: 0.2312 Val R2: 0.1145\n",
      "Epoch: 089, Train Loss: 0.0612, Val Loss: 0.0535, Val RMSE: 0.2312 Val R2: 0.1147\n",
      "Epoch: 090, Train Loss: 0.0611, Val Loss: 0.0534, Val RMSE: 0.2311 Val R2: 0.1153\n",
      "Epoch: 091, Train Loss: 0.0612, Val Loss: 0.0534, Val RMSE: 0.2310 Val R2: 0.1161\n",
      "Epoch: 092, Train Loss: 0.0610, Val Loss: 0.0533, Val RMSE: 0.2309 Val R2: 0.1171\n",
      "Epoch: 093, Train Loss: 0.0607, Val Loss: 0.0532, Val RMSE: 0.2307 Val R2: 0.1183\n",
      "Epoch: 094, Train Loss: 0.0604, Val Loss: 0.0532, Val RMSE: 0.2306 Val R2: 0.1195\n",
      "Epoch: 095, Train Loss: 0.0605, Val Loss: 0.0531, Val RMSE: 0.2304 Val R2: 0.1206\n",
      "Epoch: 096, Train Loss: 0.0601, Val Loss: 0.0530, Val RMSE: 0.2303 Val R2: 0.1218\n",
      "Epoch: 097, Train Loss: 0.0601, Val Loss: 0.0530, Val RMSE: 0.2301 Val R2: 0.1228\n",
      "Epoch: 098, Train Loss: 0.0600, Val Loss: 0.0529, Val RMSE: 0.2300 Val R2: 0.1236\n",
      "Epoch: 099, Train Loss: 0.0598, Val Loss: 0.0529, Val RMSE: 0.2300 Val R2: 0.1242\n",
      "Epoch: 100, Train Loss: 0.0598, Val Loss: 0.0529, Val RMSE: 0.2299 Val R2: 0.1245\n",
      "Epoch: 101, Train Loss: 0.0595, Val Loss: 0.0528, Val RMSE: 0.2299 Val R2: 0.1248\n",
      "Epoch: 102, Train Loss: 0.0594, Val Loss: 0.0528, Val RMSE: 0.2299 Val R2: 0.1248\n",
      "Epoch: 103, Train Loss: 0.0593, Val Loss: 0.0528, Val RMSE: 0.2299 Val R2: 0.1249\n",
      "Epoch: 104, Train Loss: 0.0594, Val Loss: 0.0528, Val RMSE: 0.2299 Val R2: 0.1249\n",
      "Epoch: 105, Train Loss: 0.0592, Val Loss: 0.0528, Val RMSE: 0.2299 Val R2: 0.1248\n",
      "Epoch: 106, Train Loss: 0.0588, Val Loss: 0.0528, Val RMSE: 0.2299 Val R2: 0.1249\n",
      "Epoch: 107, Train Loss: 0.0590, Val Loss: 0.0528, Val RMSE: 0.2298 Val R2: 0.1251\n",
      "Epoch: 108, Train Loss: 0.0589, Val Loss: 0.0528, Val RMSE: 0.2298 Val R2: 0.1254\n",
      "Epoch: 109, Train Loss: 0.0587, Val Loss: 0.0528, Val RMSE: 0.2297 Val R2: 0.1259\n",
      "Epoch: 110, Train Loss: 0.0586, Val Loss: 0.0527, Val RMSE: 0.2297 Val R2: 0.1264\n",
      "Epoch: 111, Train Loss: 0.0585, Val Loss: 0.0527, Val RMSE: 0.2296 Val R2: 0.1270\n",
      "Epoch: 112, Train Loss: 0.0583, Val Loss: 0.0527, Val RMSE: 0.2295 Val R2: 0.1276\n",
      "Epoch: 113, Train Loss: 0.0583, Val Loss: 0.0526, Val RMSE: 0.2294 Val R2: 0.1283\n",
      "Epoch: 114, Train Loss: 0.0582, Val Loss: 0.0526, Val RMSE: 0.2293 Val R2: 0.1290\n",
      "Epoch: 115, Train Loss: 0.0580, Val Loss: 0.0526, Val RMSE: 0.2292 Val R2: 0.1296\n",
      "Epoch: 116, Train Loss: 0.0581, Val Loss: 0.0525, Val RMSE: 0.2292 Val R2: 0.1301\n",
      "Epoch: 117, Train Loss: 0.0579, Val Loss: 0.0525, Val RMSE: 0.2291 Val R2: 0.1306\n",
      "Epoch: 118, Train Loss: 0.0577, Val Loss: 0.0525, Val RMSE: 0.2290 Val R2: 0.1311\n",
      "Epoch: 119, Train Loss: 0.0577, Val Loss: 0.0524, Val RMSE: 0.2290 Val R2: 0.1316\n",
      "Epoch: 120, Train Loss: 0.0577, Val Loss: 0.0524, Val RMSE: 0.2289 Val R2: 0.1321\n",
      "Epoch: 121, Train Loss: 0.0575, Val Loss: 0.0524, Val RMSE: 0.2289 Val R2: 0.1324\n",
      "Epoch: 122, Train Loss: 0.0577, Val Loss: 0.0524, Val RMSE: 0.2288 Val R2: 0.1327\n",
      "Epoch: 123, Train Loss: 0.0574, Val Loss: 0.0524, Val RMSE: 0.2288 Val R2: 0.1328\n",
      "Epoch: 124, Train Loss: 0.0575, Val Loss: 0.0523, Val RMSE: 0.2288 Val R2: 0.1330\n",
      "Epoch: 125, Train Loss: 0.0574, Val Loss: 0.0523, Val RMSE: 0.2288 Val R2: 0.1332\n",
      "Epoch: 126, Train Loss: 0.0574, Val Loss: 0.0523, Val RMSE: 0.2287 Val R2: 0.1335\n",
      "Epoch: 127, Train Loss: 0.0572, Val Loss: 0.0523, Val RMSE: 0.2287 Val R2: 0.1338\n",
      "Epoch: 128, Train Loss: 0.0572, Val Loss: 0.0523, Val RMSE: 0.2287 Val R2: 0.1341\n",
      "Epoch: 129, Train Loss: 0.0571, Val Loss: 0.0523, Val RMSE: 0.2286 Val R2: 0.1344\n",
      "Epoch: 130, Train Loss: 0.0572, Val Loss: 0.0522, Val RMSE: 0.2286 Val R2: 0.1348\n",
      "Epoch: 131, Train Loss: 0.0569, Val Loss: 0.0522, Val RMSE: 0.2285 Val R2: 0.1351\n",
      "Epoch: 132, Train Loss: 0.0568, Val Loss: 0.0522, Val RMSE: 0.2285 Val R2: 0.1355\n",
      "Epoch: 133, Train Loss: 0.0570, Val Loss: 0.0522, Val RMSE: 0.2284 Val R2: 0.1358\n",
      "Epoch: 134, Train Loss: 0.0569, Val Loss: 0.0522, Val RMSE: 0.2284 Val R2: 0.1362\n",
      "Epoch: 135, Train Loss: 0.0570, Val Loss: 0.0521, Val RMSE: 0.2283 Val R2: 0.1364\n",
      "Epoch: 136, Train Loss: 0.0568, Val Loss: 0.0521, Val RMSE: 0.2283 Val R2: 0.1367\n",
      "Epoch: 137, Train Loss: 0.0567, Val Loss: 0.0521, Val RMSE: 0.2283 Val R2: 0.1369\n",
      "Epoch: 138, Train Loss: 0.0567, Val Loss: 0.0521, Val RMSE: 0.2282 Val R2: 0.1372\n",
      "Epoch: 139, Train Loss: 0.0567, Val Loss: 0.0521, Val RMSE: 0.2282 Val R2: 0.1374\n",
      "Epoch: 140, Train Loss: 0.0566, Val Loss: 0.0521, Val RMSE: 0.2282 Val R2: 0.1375\n",
      "Epoch: 141, Train Loss: 0.0564, Val Loss: 0.0521, Val RMSE: 0.2282 Val R2: 0.1377\n",
      "Epoch: 142, Train Loss: 0.0564, Val Loss: 0.0521, Val RMSE: 0.2282 Val R2: 0.1379\n",
      "Epoch: 143, Train Loss: 0.0565, Val Loss: 0.0520, Val RMSE: 0.2281 Val R2: 0.1380\n",
      "Epoch: 144, Train Loss: 0.0565, Val Loss: 0.0520, Val RMSE: 0.2281 Val R2: 0.1382\n",
      "Epoch: 145, Train Loss: 0.0564, Val Loss: 0.0520, Val RMSE: 0.2281 Val R2: 0.1384\n",
      "Epoch: 146, Train Loss: 0.0563, Val Loss: 0.0520, Val RMSE: 0.2281 Val R2: 0.1386\n",
      "Epoch: 147, Train Loss: 0.0563, Val Loss: 0.0520, Val RMSE: 0.2280 Val R2: 0.1389\n",
      "Epoch: 148, Train Loss: 0.0564, Val Loss: 0.0520, Val RMSE: 0.2280 Val R2: 0.1391\n",
      "Epoch: 149, Train Loss: 0.0563, Val Loss: 0.0520, Val RMSE: 0.2280 Val R2: 0.1394\n",
      "Epoch: 150, Train Loss: 0.0562, Val Loss: 0.0519, Val RMSE: 0.2279 Val R2: 0.1397\n",
      "Epoch: 151, Train Loss: 0.0563, Val Loss: 0.0519, Val RMSE: 0.2279 Val R2: 0.1399\n",
      "Epoch: 152, Train Loss: 0.0563, Val Loss: 0.0519, Val RMSE: 0.2279 Val R2: 0.1402\n",
      "Epoch: 153, Train Loss: 0.0561, Val Loss: 0.0519, Val RMSE: 0.2278 Val R2: 0.1403\n",
      "Epoch: 154, Train Loss: 0.0562, Val Loss: 0.0519, Val RMSE: 0.2278 Val R2: 0.1405\n",
      "Epoch: 155, Train Loss: 0.0561, Val Loss: 0.0519, Val RMSE: 0.2278 Val R2: 0.1406\n",
      "Epoch: 156, Train Loss: 0.0560, Val Loss: 0.0519, Val RMSE: 0.2278 Val R2: 0.1407\n",
      "Epoch: 157, Train Loss: 0.0559, Val Loss: 0.0519, Val RMSE: 0.2278 Val R2: 0.1408\n",
      "Epoch: 158, Train Loss: 0.0558, Val Loss: 0.0519, Val RMSE: 0.2278 Val R2: 0.1409\n",
      "Epoch: 159, Train Loss: 0.0559, Val Loss: 0.0519, Val RMSE: 0.2277 Val R2: 0.1410\n",
      "Epoch: 160, Train Loss: 0.0560, Val Loss: 0.0519, Val RMSE: 0.2277 Val R2: 0.1410\n",
      "Epoch: 161, Train Loss: 0.0560, Val Loss: 0.0519, Val RMSE: 0.2277 Val R2: 0.1411\n",
      "Epoch: 162, Train Loss: 0.0561, Val Loss: 0.0518, Val RMSE: 0.2277 Val R2: 0.1413\n",
      "Epoch: 163, Train Loss: 0.0558, Val Loss: 0.0518, Val RMSE: 0.2277 Val R2: 0.1414\n",
      "Epoch: 164, Train Loss: 0.0558, Val Loss: 0.0518, Val RMSE: 0.2277 Val R2: 0.1416\n",
      "Epoch: 165, Train Loss: 0.0558, Val Loss: 0.0518, Val RMSE: 0.2276 Val R2: 0.1418\n",
      "Epoch: 166, Train Loss: 0.0557, Val Loss: 0.0518, Val RMSE: 0.2276 Val R2: 0.1420\n",
      "Epoch: 167, Train Loss: 0.0557, Val Loss: 0.0518, Val RMSE: 0.2276 Val R2: 0.1422\n",
      "Epoch: 168, Train Loss: 0.0557, Val Loss: 0.0518, Val RMSE: 0.2275 Val R2: 0.1425\n",
      "Epoch: 169, Train Loss: 0.0556, Val Loss: 0.0518, Val RMSE: 0.2275 Val R2: 0.1427\n",
      "Epoch: 170, Train Loss: 0.0555, Val Loss: 0.0517, Val RMSE: 0.2275 Val R2: 0.1430\n",
      "Epoch: 171, Train Loss: 0.0557, Val Loss: 0.0517, Val RMSE: 0.2275 Val R2: 0.1431\n",
      "Epoch: 172, Train Loss: 0.0556, Val Loss: 0.0517, Val RMSE: 0.2274 Val R2: 0.1433\n",
      "Epoch: 173, Train Loss: 0.0556, Val Loss: 0.0517, Val RMSE: 0.2274 Val R2: 0.1434\n",
      "Epoch: 174, Train Loss: 0.0555, Val Loss: 0.0517, Val RMSE: 0.2274 Val R2: 0.1435\n",
      "Epoch: 175, Train Loss: 0.0556, Val Loss: 0.0517, Val RMSE: 0.2274 Val R2: 0.1437\n",
      "Epoch: 176, Train Loss: 0.0556, Val Loss: 0.0517, Val RMSE: 0.2274 Val R2: 0.1438\n",
      "Epoch: 177, Train Loss: 0.0555, Val Loss: 0.0517, Val RMSE: 0.2274 Val R2: 0.1438\n",
      "Epoch: 178, Train Loss: 0.0555, Val Loss: 0.0517, Val RMSE: 0.2274 Val R2: 0.1439\n",
      "Epoch: 179, Train Loss: 0.0554, Val Loss: 0.0517, Val RMSE: 0.2274 Val R2: 0.1439\n",
      "Epoch: 180, Train Loss: 0.0554, Val Loss: 0.0517, Val RMSE: 0.2273 Val R2: 0.1440\n",
      "Epoch: 181, Train Loss: 0.0554, Val Loss: 0.0517, Val RMSE: 0.2273 Val R2: 0.1441\n",
      "Epoch: 182, Train Loss: 0.0555, Val Loss: 0.0517, Val RMSE: 0.2273 Val R2: 0.1442\n",
      "Epoch: 183, Train Loss: 0.0555, Val Loss: 0.0517, Val RMSE: 0.2273 Val R2: 0.1443\n",
      "Epoch: 184, Train Loss: 0.0555, Val Loss: 0.0517, Val RMSE: 0.2273 Val R2: 0.1445\n",
      "Epoch: 185, Train Loss: 0.0553, Val Loss: 0.0516, Val RMSE: 0.2273 Val R2: 0.1447\n",
      "Epoch: 186, Train Loss: 0.0552, Val Loss: 0.0516, Val RMSE: 0.2272 Val R2: 0.1449\n",
      "Epoch: 187, Train Loss: 0.0552, Val Loss: 0.0516, Val RMSE: 0.2272 Val R2: 0.1451\n",
      "Epoch: 188, Train Loss: 0.0555, Val Loss: 0.0516, Val RMSE: 0.2272 Val R2: 0.1453\n",
      "Epoch: 189, Train Loss: 0.0552, Val Loss: 0.0516, Val RMSE: 0.2271 Val R2: 0.1455\n",
      "Epoch: 190, Train Loss: 0.0553, Val Loss: 0.0516, Val RMSE: 0.2271 Val R2: 0.1457\n",
      "Epoch: 191, Train Loss: 0.0554, Val Loss: 0.0516, Val RMSE: 0.2271 Val R2: 0.1458\n",
      "Epoch: 192, Train Loss: 0.0553, Val Loss: 0.0516, Val RMSE: 0.2271 Val R2: 0.1459\n",
      "Epoch: 193, Train Loss: 0.0553, Val Loss: 0.0516, Val RMSE: 0.2271 Val R2: 0.1460\n",
      "Epoch: 194, Train Loss: 0.0552, Val Loss: 0.0516, Val RMSE: 0.2271 Val R2: 0.1460\n",
      "Epoch: 195, Train Loss: 0.0552, Val Loss: 0.0516, Val RMSE: 0.2271 Val R2: 0.1461\n",
      "Epoch: 196, Train Loss: 0.0554, Val Loss: 0.0516, Val RMSE: 0.2271 Val R2: 0.1461\n",
      "Epoch: 197, Train Loss: 0.0552, Val Loss: 0.0516, Val RMSE: 0.2271 Val R2: 0.1461\n",
      "Epoch: 198, Train Loss: 0.0552, Val Loss: 0.0516, Val RMSE: 0.2271 Val R2: 0.1462\n",
      "Epoch: 199, Train Loss: 0.0550, Val Loss: 0.0515, Val RMSE: 0.2270 Val R2: 0.1463\n",
      "Epoch: 200, Train Loss: 0.0553, Val Loss: 0.0515, Val RMSE: 0.2270 Val R2: 0.1464\n",
      "Epoch: 201, Train Loss: 0.0553, Val Loss: 0.0515, Val RMSE: 0.2270 Val R2: 0.1466\n",
      "Epoch: 202, Train Loss: 0.0551, Val Loss: 0.0515, Val RMSE: 0.2270 Val R2: 0.1468\n",
      "Epoch: 203, Train Loss: 0.0551, Val Loss: 0.0515, Val RMSE: 0.2270 Val R2: 0.1469\n",
      "Epoch: 204, Train Loss: 0.0550, Val Loss: 0.0515, Val RMSE: 0.2269 Val R2: 0.1471\n",
      "Epoch: 205, Train Loss: 0.0551, Val Loss: 0.0515, Val RMSE: 0.2269 Val R2: 0.1472\n",
      "Epoch: 206, Train Loss: 0.0550, Val Loss: 0.0515, Val RMSE: 0.2269 Val R2: 0.1473\n",
      "Epoch: 207, Train Loss: 0.0551, Val Loss: 0.0515, Val RMSE: 0.2269 Val R2: 0.1474\n",
      "Epoch: 208, Train Loss: 0.0550, Val Loss: 0.0515, Val RMSE: 0.2269 Val R2: 0.1476\n",
      "Epoch: 209, Train Loss: 0.0549, Val Loss: 0.0515, Val RMSE: 0.2269 Val R2: 0.1477\n",
      "Epoch: 210, Train Loss: 0.0550, Val Loss: 0.0515, Val RMSE: 0.2268 Val R2: 0.1478\n",
      "Epoch: 211, Train Loss: 0.0550, Val Loss: 0.0514, Val RMSE: 0.2268 Val R2: 0.1479\n",
      "Epoch: 212, Train Loss: 0.0549, Val Loss: 0.0514, Val RMSE: 0.2268 Val R2: 0.1480\n",
      "Epoch: 213, Train Loss: 0.0551, Val Loss: 0.0514, Val RMSE: 0.2268 Val R2: 0.1481\n",
      "Epoch: 214, Train Loss: 0.0549, Val Loss: 0.0514, Val RMSE: 0.2268 Val R2: 0.1481\n",
      "Epoch: 215, Train Loss: 0.0550, Val Loss: 0.0514, Val RMSE: 0.2268 Val R2: 0.1482\n",
      "Epoch: 216, Train Loss: 0.0549, Val Loss: 0.0514, Val RMSE: 0.2268 Val R2: 0.1482\n",
      "Epoch: 217, Train Loss: 0.0549, Val Loss: 0.0514, Val RMSE: 0.2268 Val R2: 0.1482\n",
      "Epoch: 218, Train Loss: 0.0548, Val Loss: 0.0514, Val RMSE: 0.2268 Val R2: 0.1483\n",
      "Epoch: 219, Train Loss: 0.0549, Val Loss: 0.0514, Val RMSE: 0.2268 Val R2: 0.1484\n",
      "Epoch: 220, Train Loss: 0.0548, Val Loss: 0.0514, Val RMSE: 0.2267 Val R2: 0.1485\n",
      "Epoch: 221, Train Loss: 0.0550, Val Loss: 0.0514, Val RMSE: 0.2267 Val R2: 0.1485\n",
      "Epoch: 222, Train Loss: 0.0547, Val Loss: 0.0514, Val RMSE: 0.2267 Val R2: 0.1487\n",
      "Epoch: 223, Train Loss: 0.0548, Val Loss: 0.0514, Val RMSE: 0.2267 Val R2: 0.1488\n",
      "Epoch: 224, Train Loss: 0.0549, Val Loss: 0.0514, Val RMSE: 0.2267 Val R2: 0.1489\n",
      "Epoch: 225, Train Loss: 0.0547, Val Loss: 0.0514, Val RMSE: 0.2267 Val R2: 0.1491\n",
      "Epoch: 226, Train Loss: 0.0549, Val Loss: 0.0514, Val RMSE: 0.2267 Val R2: 0.1492\n",
      "Epoch: 227, Train Loss: 0.0549, Val Loss: 0.0514, Val RMSE: 0.2266 Val R2: 0.1493\n",
      "Epoch: 228, Train Loss: 0.0547, Val Loss: 0.0514, Val RMSE: 0.2266 Val R2: 0.1494\n",
      "Epoch: 229, Train Loss: 0.0548, Val Loss: 0.0514, Val RMSE: 0.2266 Val R2: 0.1495\n",
      "Epoch: 230, Train Loss: 0.0548, Val Loss: 0.0513, Val RMSE: 0.2266 Val R2: 0.1496\n",
      "Epoch: 231, Train Loss: 0.0546, Val Loss: 0.0513, Val RMSE: 0.2266 Val R2: 0.1497\n",
      "Epoch: 232, Train Loss: 0.0548, Val Loss: 0.0513, Val RMSE: 0.2266 Val R2: 0.1498\n",
      "Epoch: 233, Train Loss: 0.0547, Val Loss: 0.0513, Val RMSE: 0.2266 Val R2: 0.1499\n",
      "Epoch: 234, Train Loss: 0.0547, Val Loss: 0.0513, Val RMSE: 0.2265 Val R2: 0.1501\n",
      "Epoch: 235, Train Loss: 0.0546, Val Loss: 0.0513, Val RMSE: 0.2265 Val R2: 0.1502\n",
      "Epoch: 236, Train Loss: 0.0548, Val Loss: 0.0513, Val RMSE: 0.2265 Val R2: 0.1503\n",
      "Epoch: 237, Train Loss: 0.0549, Val Loss: 0.0513, Val RMSE: 0.2265 Val R2: 0.1503\n",
      "Epoch: 238, Train Loss: 0.0547, Val Loss: 0.0513, Val RMSE: 0.2265 Val R2: 0.1504\n",
      "Epoch: 239, Train Loss: 0.0547, Val Loss: 0.0513, Val RMSE: 0.2265 Val R2: 0.1504\n",
      "Epoch: 240, Train Loss: 0.0547, Val Loss: 0.0513, Val RMSE: 0.2265 Val R2: 0.1505\n",
      "Epoch: 241, Train Loss: 0.0547, Val Loss: 0.0513, Val RMSE: 0.2265 Val R2: 0.1505\n",
      "Epoch: 242, Train Loss: 0.0547, Val Loss: 0.0513, Val RMSE: 0.2265 Val R2: 0.1505\n",
      "Epoch: 243, Train Loss: 0.0547, Val Loss: 0.0513, Val RMSE: 0.2265 Val R2: 0.1505\n",
      "Epoch: 244, Train Loss: 0.0547, Val Loss: 0.0513, Val RMSE: 0.2265 Val R2: 0.1506\n",
      "Epoch: 245, Train Loss: 0.0547, Val Loss: 0.0513, Val RMSE: 0.2265 Val R2: 0.1506\n",
      "Epoch: 246, Train Loss: 0.0547, Val Loss: 0.0513, Val RMSE: 0.2265 Val R2: 0.1507\n",
      "Epoch: 247, Train Loss: 0.0548, Val Loss: 0.0513, Val RMSE: 0.2265 Val R2: 0.1507\n",
      "Epoch: 248, Train Loss: 0.0546, Val Loss: 0.0513, Val RMSE: 0.2265 Val R2: 0.1507\n",
      "Epoch: 249, Train Loss: 0.0547, Val Loss: 0.0513, Val RMSE: 0.2264 Val R2: 0.1507\n",
      "Epoch: 250, Train Loss: 0.0546, Val Loss: 0.0513, Val RMSE: 0.2264 Val R2: 0.1507\n",
      "Epoch: 251, Train Loss: 0.0547, Val Loss: 0.0513, Val RMSE: 0.2264 Val R2: 0.1507\n",
      "Epoch: 252, Train Loss: 0.0545, Val Loss: 0.0513, Val RMSE: 0.2264 Val R2: 0.1508\n",
      "Epoch: 253, Train Loss: 0.0547, Val Loss: 0.0513, Val RMSE: 0.2264 Val R2: 0.1509\n",
      "Epoch: 254, Train Loss: 0.0546, Val Loss: 0.0513, Val RMSE: 0.2264 Val R2: 0.1509\n",
      "Epoch: 255, Train Loss: 0.0547, Val Loss: 0.0513, Val RMSE: 0.2264 Val R2: 0.1509\n",
      "Epoch: 256, Train Loss: 0.0546, Val Loss: 0.0513, Val RMSE: 0.2264 Val R2: 0.1509\n",
      "Epoch: 257, Train Loss: 0.0545, Val Loss: 0.0513, Val RMSE: 0.2264 Val R2: 0.1510\n",
      "Epoch: 258, Train Loss: 0.0544, Val Loss: 0.0513, Val RMSE: 0.2264 Val R2: 0.1511\n",
      "Epoch: 259, Train Loss: 0.0545, Val Loss: 0.0512, Val RMSE: 0.2264 Val R2: 0.1513\n",
      "Epoch: 260, Train Loss: 0.0545, Val Loss: 0.0512, Val RMSE: 0.2263 Val R2: 0.1515\n",
      "Epoch: 261, Train Loss: 0.0546, Val Loss: 0.0512, Val RMSE: 0.2263 Val R2: 0.1517\n",
      "Epoch: 262, Train Loss: 0.0546, Val Loss: 0.0512, Val RMSE: 0.2263 Val R2: 0.1518\n",
      "Epoch: 263, Train Loss: 0.0545, Val Loss: 0.0512, Val RMSE: 0.2263 Val R2: 0.1520\n",
      "Epoch: 264, Train Loss: 0.0545, Val Loss: 0.0512, Val RMSE: 0.2263 Val R2: 0.1521\n",
      "Epoch: 265, Train Loss: 0.0546, Val Loss: 0.0512, Val RMSE: 0.2263 Val R2: 0.1522\n",
      "Epoch: 266, Train Loss: 0.0545, Val Loss: 0.0512, Val RMSE: 0.2262 Val R2: 0.1523\n",
      "Epoch: 267, Train Loss: 0.0546, Val Loss: 0.0512, Val RMSE: 0.2262 Val R2: 0.1523\n",
      "Epoch: 268, Train Loss: 0.0544, Val Loss: 0.0512, Val RMSE: 0.2262 Val R2: 0.1525\n",
      "Epoch: 269, Train Loss: 0.0545, Val Loss: 0.0512, Val RMSE: 0.2262 Val R2: 0.1526\n",
      "Epoch: 270, Train Loss: 0.0545, Val Loss: 0.0512, Val RMSE: 0.2262 Val R2: 0.1526\n",
      "Epoch: 271, Train Loss: 0.0545, Val Loss: 0.0512, Val RMSE: 0.2262 Val R2: 0.1527\n",
      "Epoch: 272, Train Loss: 0.0544, Val Loss: 0.0512, Val RMSE: 0.2262 Val R2: 0.1528\n",
      "Epoch: 273, Train Loss: 0.0544, Val Loss: 0.0512, Val RMSE: 0.2262 Val R2: 0.1529\n",
      "Epoch: 274, Train Loss: 0.0544, Val Loss: 0.0511, Val RMSE: 0.2262 Val R2: 0.1529\n",
      "Epoch: 275, Train Loss: 0.0546, Val Loss: 0.0511, Val RMSE: 0.2262 Val R2: 0.1529\n",
      "Epoch: 276, Train Loss: 0.0544, Val Loss: 0.0511, Val RMSE: 0.2262 Val R2: 0.1529\n",
      "Epoch: 277, Train Loss: 0.0543, Val Loss: 0.0511, Val RMSE: 0.2261 Val R2: 0.1530\n",
      "Epoch: 278, Train Loss: 0.0545, Val Loss: 0.0511, Val RMSE: 0.2261 Val R2: 0.1531\n",
      "Epoch: 279, Train Loss: 0.0544, Val Loss: 0.0511, Val RMSE: 0.2261 Val R2: 0.1532\n",
      "Epoch: 280, Train Loss: 0.0543, Val Loss: 0.0511, Val RMSE: 0.2261 Val R2: 0.1532\n",
      "Epoch: 281, Train Loss: 0.0543, Val Loss: 0.0511, Val RMSE: 0.2261 Val R2: 0.1533\n",
      "Epoch: 282, Train Loss: 0.0543, Val Loss: 0.0511, Val RMSE: 0.2261 Val R2: 0.1534\n",
      "Epoch: 283, Train Loss: 0.0544, Val Loss: 0.0511, Val RMSE: 0.2261 Val R2: 0.1535\n",
      "Epoch: 284, Train Loss: 0.0544, Val Loss: 0.0511, Val RMSE: 0.2261 Val R2: 0.1536\n",
      "Epoch: 285, Train Loss: 0.0544, Val Loss: 0.0511, Val RMSE: 0.2261 Val R2: 0.1537\n",
      "Epoch: 286, Train Loss: 0.0543, Val Loss: 0.0511, Val RMSE: 0.2260 Val R2: 0.1538\n",
      "Epoch: 287, Train Loss: 0.0543, Val Loss: 0.0511, Val RMSE: 0.2260 Val R2: 0.1539\n",
      "Epoch: 288, Train Loss: 0.0545, Val Loss: 0.0511, Val RMSE: 0.2260 Val R2: 0.1539\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 40\u001b[0m\n\u001b[0;32m     37\u001b[0m val_rmses \u001b[39m=\u001b[39m []\n\u001b[0;32m     39\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[1;32m---> 40\u001b[0m     train_loss \u001b[39m=\u001b[39m train_model(model, data, data\u001b[39m.\u001b[39;49mtrain_mask)\n\u001b[0;32m     41\u001b[0m     val_mse, val_rmse, val_mae, r2 \u001b[39m=\u001b[39m evaluate_model(model, data, data\u001b[39m.\u001b[39mval_mask)\n\u001b[0;32m     43\u001b[0m     train_losses\u001b[39m.\u001b[39mappend(train_loss)\n",
      "Cell \u001b[1;32mIn[12], line 17\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, data, mask)\u001b[0m\n\u001b[0;32m     15\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m     16\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> 17\u001b[0m out \u001b[39m=\u001b[39m model(data\u001b[39m.\u001b[39;49mx, data\u001b[39m.\u001b[39;49medge_index, data\u001b[39m.\u001b[39;49medge_attributes)[mask]\n\u001b[0;32m     18\u001b[0m loss \u001b[39m=\u001b[39m criterion(out, data\u001b[39m.\u001b[39my[mask]\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m))\n\u001b[0;32m     19\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\OneDrive - Universidad Católica de Chile\\Documentos\\Tesis_Master\\tesisenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\OneDrive - Universidad Católica de Chile\\Documentos\\Tesis_Master\\Models\\GATv2.py:55\u001b[0m, in \u001b[0;36mGATv2Regression.forward\u001b[1;34m(self, x, edge_index, edge_attr)\u001b[0m\n\u001b[0;32m     53\u001b[0m x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39melu(x)\n\u001b[0;32m     54\u001b[0m \u001b[39mfor\u001b[39;00m conv \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconvs:\n\u001b[1;32m---> 55\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49melu(conv(x, edge_index))\n\u001b[0;32m     56\u001b[0m x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mdropout(x, p\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, training\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining)\n\u001b[0;32m     57\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2(x, edge_index)\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\OneDrive - Universidad Católica de Chile\\Documentos\\Tesis_Master\\tesisenv\\Lib\\site-packages\\torch\\nn\\functional.py:1548\u001b[0m, in \u001b[0;36melu\u001b[1;34m(input, alpha, inplace)\u001b[0m\n\u001b[0;32m   1546\u001b[0m     result \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_nn\u001b[39m.\u001b[39melu_(\u001b[39minput\u001b[39m, alpha)\n\u001b[0;32m   1547\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1548\u001b[0m     result \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49melu(\u001b[39minput\u001b[39;49m, alpha)\n\u001b[0;32m   1549\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "in_features = data.x.shape[1]\n",
    "hidden_features = 16\n",
    "out_features = 1\n",
    "num_layers = 2\n",
    "epochs = 300\n",
    "lr = 0.01\n",
    "num_heads = 6\n",
    "\n",
    "model = GATv2Regression(in_features, hidden_features, out_features, edge_dim=2, heads=num_heads, num_layers=2)\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "def train_model(model, data, mask):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index, data.edge_attributes)[mask]\n",
    "    loss = criterion(out, data.y[mask].view(-1, 1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "def evaluate_model(model, data, mask):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data.x, data.edge_index, data.edge_attributes)[mask]\n",
    "        mse = criterion(out, data.y[mask].view(-1, 1))\n",
    "        r2 = r2_score(data.y[mask].view(-1, 1), out)\n",
    "        mae = mean_absolute_error(out, data.y[mask].view(-1, 1))\n",
    "        rmse = torch.sqrt(mse)\n",
    "    return mse.item(), rmse, mae, r2\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_maes = []\n",
    "val_maes = []\n",
    "val_rmses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train_model(model, data, data.train_mask)\n",
    "    val_mse, val_rmse, val_mae, r2 = evaluate_model(model, data, data.val_mask)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_mse)\n",
    "    val_maes.append(val_mae)\n",
    "    val_rmses.append(val_rmse)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:03d}, Train Loss: {train_loss:.4f}, Val Loss: {val_mse:.4f}, Val RMSE: {val_rmse:.4f} Val R2: {r2:.4f}')\n",
    "\n",
    "test_mse, test_rmse, test_mae, test_r2 = evaluate_model(model, data, data.test_mask)\n",
    "print(f'Test RMSE: {test_rmse:.4f}, Test MAE: {test_mae:.4f}, Test R2: {test_r2:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tensor = torch.cat((data.x, data.y.unsqueeze(1), data.train_mask.unsqueeze(1), data.test_mask.unsqueeze(1)), dim=1)\n",
    "df_ismt = pd.DataFrame(data_tensor.numpy(), columns=['beautiful','boring','depressing','lively','safe','wealthy',\n",
    "                                                     'pct_hog40p', 'train', 'test'])\n",
    "\n",
    "train_split = df_ismt[df_ismt.train == 1]\n",
    "test_split = df_ismt[df_ismt.test == 1]\n",
    "\n",
    "X_train, y_train = train_split[['beautiful','boring','depressing','lively','safe','wealthy']], train_split.pct_hog40p\n",
    "X_test, y_test = test_split[['beautiful','boring','depressing','lively','safe','wealthy']], test_split.pct_hog40p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.0564\n",
      "Mean Absolute Error: 0.2046\n",
      "R2 Score: 0.1067\n"
     ]
    }
   ],
   "source": [
    "reg = LinearRegression()\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "y_pred = reg.predict(X_test)\n",
    "\n",
    "mse_error = mean_squared_error(y_test, y_pred)\n",
    "mae_error = mean_absolute_error(y_test, y_pred)\n",
    "r2_score_error = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse_error:.4f}\")\n",
    "print(f\"Mean Absolute Error: {mae_error:.4f}\")\n",
    "print(f\"R2 Score: {r2_score_error:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.0691\n",
      "Mean Absolute Error: 0.1772\n",
      "R2 Score: -0.0934\n",
      "CPU times: total: 3.77 s\n",
      "Wall time: 13.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "reg = DecisionTreeRegressor()\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "y_pred = reg.predict(X_test)\n",
    "\n",
    "mse_error = mean_squared_error(y_test, y_pred)\n",
    "mae_error = mean_absolute_error(y_test, y_pred)\n",
    "r2_score_error = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse_error:.4f}\")\n",
    "print(f\"Mean Absolute Error: {mae_error:.4f}\")\n",
    "print(f\"R2 Score: {r2_score_error:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tesisenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
